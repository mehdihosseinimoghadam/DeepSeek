<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Alignment: PPO, DPO, and GRPO</title>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600;700&family=Roboto:wght@400;500&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #2980b9;
            --secondary-color: #27ae60;
            --accent-color: #f39c12;
            --bg-color: #f4f7f9;
            --text-color: #34495e;
            --container-bg: #ffffff;
            --header-bg: #2c3e50;
            --header-text: #ecf0f1;
            --code-bg: #2d3436;
            --code-text: #dfe6e9;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        body {
            font-family: 'Roboto', sans-serif;
            line-height: 1.7;
            color: var(--text-color);
            background-color: var(--bg-color);
            margin: 0;
            padding: 0;
        }

        header {
            background: var(--header-bg);
            color: var(--header-text);
            padding: 40px 20px;
            text-align: center;
        }
        
        header h1 {
            margin: 0;
            font-size: 2.8em;
            font-weight: 700;
            border: none;
            color: var(--header-text);
        }

        .main-content {
            max-width: 1400px;
            margin: 0 auto;
            padding: 20px;
        }

        h1, h2, h3, h4 {
            font-family: 'Montserrat', sans-serif;
            color: var(--header-bg);
        }

        h2, h3, h4 {
            border-bottom: 3px solid var(--primary-color);
            padding-bottom: 10px;
            margin-top: 40px;
        }
        
        h4 {
            border-bottom-width: 2px;
            margin-top: 25px;
        }

        code, .mono {
            background-color: #ecf0f1;
            padding: 3px 6px;
            border-radius: 4px;
            font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace;
            font-size: 0.9em;
        }

        .container {
            background-color: var(--container-bg);
            padding: 30px;
            border-radius: 12px;
            box-shadow: 0 8px 16px rgba(0,0,0,0.05), 0 3px 6px rgba(0,0,0,0.04);
            margin-bottom: 30px;
            animation: fadeIn 0.5s ease-out forwards;
        }

        .equation {
            background-color: #eaf2f8;
            border-left: 5px solid var(--primary-color);
            padding: 20px;
            margin: 25px 0;
            font-style: italic;
            text-align: center;
            font-size: 1.15em;
            border-radius: 0 8px 8px 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 25px;
            font-size: 0.9em; /* Reduced font size */
            table-layout: fixed; /* Added for better layout control */
        }

        th, td {
            border: 1px solid #dfe6e9;
            padding: 12px; /* Reduced padding */
            text-align: left;
            vertical-align: top;
            word-wrap: break-word; /* Ensure text wrapping */
        }

        th {
            background-color: var(--primary-color);
            color: white;
            font-weight: 600;
        }

        tr:nth-child(even) {
            background-color: #f8f9fa;
        }

        tr:hover {
            background-color: #e9ecef;
        }

        .math-jax {
            font-size: 1.1em;
        }

        .example-box {
            border: 1px solid #dfe6e9;
            border-radius: 8px;
            padding: 20px;
            margin-top: 20px;
            background-color: #fdfdfd;
            box-shadow: 0 2px 4px rgba(0,0,0,0.04);
        }

        .math-cell {
            text-align: center;
            vertical-align: middle;
            font-size: 1em;
            background-color: #f8f9fa;
            word-break: break-all; /* Allow breaking long math strings */
        }

        /* --- Custom Diagram Styles --- */
        .workflow-arrow-down {
            text-align: center;
            font-size: 2em;
            color: var(--primary-color);
            margin: 8px 0;
            font-weight: bold;
        }
        .workflow-step {
            background: var(--container-bg);
            border: 2px solid #bdc3c7;
            border-radius: 8px;
            padding: 15px;
            text-align: center;
            margin: 10px auto;
            transition: transform 0.2s, box-shadow 0.2s;
            max-width: 95%;
        }
        .workflow-step:hover {
            transform: translateY(-3px);
            box-shadow: 0 6px 12px rgba(0,0,0,0.1);
        }
        .workflow-step.final { background-color: #d4efdf; border-color: var(--secondary-color); font-weight: bold; }
        .workflow-step.output { background-color: #fef5e7; border-color: var(--accent-color); }

        /* PPO Interactive Workflow */
        #ppo-interactive-workflow {
            background: #fdfdfd;
            padding: 25px 25px 0 25px;
            border-radius: 12px;
            margin-top: 20px;
            border: 1px solid #ecf0f1;
            overflow: visible;
        }

        .card-container {
            position: relative;
            height: 750px;
            display: flex;
            align-items: flex-start;
            padding: 15px 0;
            margin-bottom: 0;
            overflow: hidden;
        }

        .workflow-card {
            position: absolute;
            width: 100%;
            height: 100%;
            opacity: 0;
            transform: translateX(40px);
            transition: opacity 0.4s ease-in-out, transform 0.4s ease-in-out;
            pointer-events: none;
            padding: 0 10px 20px 10px;
            overflow-y: auto;
            box-sizing: border-box;
        }

        .workflow-card.active {
            opacity: 1;
            transform: translateX(0);
            pointer-events: auto;
        }

        .workflow-card h5 {
            font-size: 1.4em;
            color: var(--primary-color);
            margin-bottom: 15px;
            border: none;
            font-weight: 600;
            text-align: center;
        }
        
        /* Compact Workflow Steps */
        .workflow-mini {
            display: flex;
            flex-direction: column;
            align-items: center;
            max-width: 500px;
            margin: 0 auto 15px auto;
        }
        
        .workflow-card .workflow-step {
            font-size: 0.85em;
            padding: 8px 16px;
            margin: 4px auto;
            max-width: 100%;
            border-radius: 25px;
            border-width: 1px;
            transition: all 0.2s ease;
        }
        
        .workflow-card .workflow-step:hover {
            transform: translateY(-1px);
            box-shadow: 0 3px 6px rgba(0,0,0,0.08);
        }
        
        .workflow-card .workflow-arrow-down {
            font-size: 1.2em;
            margin: 3px 0;
            color: var(--primary-color);
        }
        
        .card-explainer {
            margin: 10px 0 15px 0;
            font-size: 0.9em;
            color: #666;
            padding: 0 8%;
            line-height: 1.4;
            text-align: center;
        }

        /* Two Column Layout for Math and Examples */
        .card-content-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-top: 15px;
            max-width: 98%;
            margin-left: auto;
            margin-right: auto;
        }

        .card-math-section {
            padding: 18px;
            background-color: #f8f9fa;
            border-radius: 8px;
            border-left: 4px solid var(--primary-color);
        }

        .card-math-section h6 {
            color: var(--primary-color);
            margin: 0 0 10px 0;
            font-size: 1em;
            border: none;
            font-weight: 600;
        }

        .card-math-section p {
            font-size: 0.85em;
            margin-bottom: 8px;
        }

        .card-example {
            padding: 18px;
            background-color: #fff;
            border-radius: 8px;
            border: 1px solid #ddd;
            text-align: left;
            font-size: 0.85em;
            line-height: 1.4;
        }

        .card-example h6 {
            color: var(--accent-color);
            margin: 0 0 10px 0;
            font-size: 1em;
            border: none;
            font-weight: 600;
        }

        .card-navigation {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 20px 25px;
            border-top: 2px solid #eee;
            background-color: #fdfdfd;
            margin: 0 -25px;
            position: relative;
            z-index: 100;
        }

        .card-navigation button {
            background-color: var(--primary-color);
            color: white;
            border: none;
            padding: 12px 25px;
            border-radius: 8px;
            cursor: pointer;
            font-weight: 600;
            font-size: 1em;
            transition: background-color 0.2s, transform 0.1s;
            min-width: 100px;
        }

        .card-navigation button:hover:not(:disabled) {
            background-color: #3498db;
            transform: translateY(-1px);
        }

        .card-navigation button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
            transform: none;
        }

        .step-indicators {
            display: flex;
            gap: 12px;
        }

        .step-indicators .dot {
            width: 14px;
            height: 14px;
            background-color: #ccc;
            border-radius: 50%;
            transition: background-color 0.3s, transform 0.2s;
            cursor: pointer;
        }

        .step-indicators .dot.active {
            background-color: var(--primary-color);
            transform: scale(1.1);
        }

        .step-indicators .dot:hover {
            transform: scale(1.05);
        }

        /* Responsive adjustments */
        @media (max-width: 1200px) {
            .main-content {
                max-width: 95%;
                padding: 15px;
            }
        }

        @media (max-width: 768px) {
            .card-content-grid {
                grid-template-columns: 1fr;
                gap: 10px;
            }
            
            .workflow-mini {
                max-width: 100%;
            }
            
            .card-explainer {
                padding: 0 5%;
            }
            
            .card-container {
                height: 500px;
            }
            
            .main-content {
                max-width: 100%;
                padding: 10px;
            }
        }

        /* General 2-Column Layout */
        .two-col-layout {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            align-items: flex-start;
        }
        .two-col-layout .col {
            background: #f9f9f9;
            padding: 20px;
            border-radius: 8px;
            border: 1px solid #ecf0f1;
        }
        .two-col-layout h4 { text-align: center; margin-top: 0; border-bottom: none; }


        /* 3-Column Layout for PPO */
        .workflow-container-3col {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 25px;
        }
        .workflow-container-3col .workflow-col {
            flex: 1;
            background: #fdfdfd;
            padding: 15px;
            border-radius: 8px;
            border: 1px solid #ecf0f1;
        }
        .workflow-container-3col h4 { text-align: center; margin-top: 0; border-bottom: none; padding-bottom: 5px; }
        .workflow-container-3col .workflow-step.output { background-color: #fef5e7; border-color: var(--accent-color); }

        /* 2-Column Layout for DPO vs PPO */
        .workflow-container-2col {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 35px;
        }
        .workflow-container-2col .workflow-col { flex: 1; background: #fdfdfd; padding: 15px; border-radius: 8px; border: 1px solid #ecf0f1; }
        .workflow-container-2col h4 { text-align: center; margin-top: 0; border-bottom: none; }
        .workflow-step.highlight1 { background-color: #f9e79f; border-color: #f1c40f; }
        .workflow-step.highlight2 { background-color: #d2b4de; border-color: #8e44ad; }
        .workflow-step.highlight3 { background-color: #aed6f1; border-color: var(--primary-color); }

        /* GRPO Workflow Layout */
        .grpo-workflow-container { display: flex; flex-direction: column; align-items: center; }
        .completions-container { display: flex; justify-content: space-around; width: 100%; gap: 10px; margin: 10px 0; }
        .completions-container .workflow-step { flex: 1; background-color: #e5e8e8; border-color: #95a5a6; margin: 0; }

        /* Full Pipeline Diagram Styles */
        .full-pipeline-container { display: grid; grid-template-columns: 1fr 0.1fr 1fr 0.1fr 1fr; gap: 20px; align-items: flex-start; }
        .pipeline-stage { flex: 1; background: #fdfdfd; padding: 15px; border-radius: 8px; border: 1px solid #ecf0f1; text-align: center; height: 100%;}
        .pipeline-stage h4 { border-bottom: 2px solid #999; }
        .pipeline-arrow { font-size: 2.5em; color: var(--primary-color); text-align: center; margin-top: 150px; }
        .workflow-step.highlight-start { background-color: #eaf2f8; border-color: #aed6f1; }
        .workflow-step.highlight-loop { background-color: #fdebd0; border-color: #f5b041; border-style: dashed; }
        .workflow-step.highlight-out { background-color: #e8f8f5; border-color: #1abc9c; font-weight: bold; }
        .workflow-step.highlight-data { background-color: #f5eef8; border-color: #9b59b6; }
        .workflow-step.highlight-sft { background-color: #d6eaf8; border-color: var(--primary-color); }
        .code-block { background-color: var(--code-bg); color: var(--code-text); padding: 15px; border-radius: 5px; text-align: left; font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace; margin-top: 10px; }


        footer {
            text-align: center;
            padding: 30px 20px;
            margin-top: 40px;
            background-color: var(--header-bg);
            color: var(--header-text);
            font-size: 0.9em;
        }

    </style>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
</head>
<body>
    <header>
        <h1>Aligning Large Language Models</h1>
    </header>

    <div class="main-content">
        <div class="container">
            <h2>Introduction</h2>
            <p>Welcome! This lecture explores the critical challenge of <strong>aligning Large Language Models (LLMs)</strong>. While modern LLMs can write code, compose poetry, and answer complex questions, their raw, pre-trained versions may not always behave as we'd like. They can sometimes generate incorrect, biased, or unhelpful content. Alignment is the process of fine-tuning these models to be helpful, harmless, and consistent with human values.</p>
            
            <p>The most common framework for this is <strong>Reinforcement Learning from Human Feedback (RLHF)</strong>. At its core, RLHF is a sophisticated process for teaching an AI to understand and adopt human preferences. It typically involves three key stages:</p>
            <ol>
                <li><strong>Supervised Fine-Tuning (SFT):</strong> A pre-trained LLM is first taught to mimic human-written examples for specific tasks, creating a solid baseline model (often called the reference policy, \(\pi_{\text{ref}}\)).</li>
                <li><strong>Reward Modeling:</strong> Humans are asked to rank different model responses to the same prompt. This preference data (\(y_w \succ y_l\), where response \(y_w\) is preferred over \(y_l\)) is used to train a separate "reward model" that learns to score responses based on how much a human would like them.</li>
                <li><strong>RL Fine-Tuning:</strong> The SFT model is further optimized using reinforcement learning. It generates responses, which are scored by the reward model. This score (or "reward") is used to update the LLM's policy, encouraging it to produce more high-reward outputs.</li>
            </ol>
    
            <p>In this lecture, we will journey through three key algorithms that tackle the RL fine-tuning stage, each with a unique approach to this complex optimization problem: <strong>PPO</strong>, the classic workhorse; <strong>DPO</strong>, a clever and more direct method; and <strong>GRPO</strong>, an efficient, critic-free alternative pioneered by recent research.</p>
        </div>
    
        <div class="container">
            <h2>1. The Classic Approach: Proximal Policy Optimization (PPO)</h2>
            <p>The traditional RLHF pipeline relies on PPO for its final and most critical phase. It carefully updates the language model based on feedback from a separately trained reward model. Below is an interactive walkthrough of its three core phases.</p>
    
            <div id="ppo-interactive-workflow">
                <div class="card-container">
                    <div class="workflow-card active" data-step="1">
                        <h5>Mathematical Foundations: Policy Gradient Methods</h5>
                        <p class="card-explainer">PPO builds on the Policy Gradient Theorem, which provides the mathematical foundation for optimizing policies in reinforcement learning.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>Policy Gradient Theorem: The Foundation of Learning</h6>
                                <p><strong>The Big Picture:</strong> Imagine teaching a student to write better essays. When they write something good, you encourage more of that behavior. When they write something bad, you discourage it. That's exactly what policy gradients do for LLMs!</p>
                                
                                <p><strong>The Mathematical Heart:</strong></p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ \nabla_\theta J(\theta) = \hat{\mathbb{E}}_t [ \nabla_\theta \log \pi_\theta(a_t | s_t) \hat{A}_t ] $$
                                </div>
                                
                                <p><strong>Let's break this down piece by piece:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>\(\theta\):</strong> The model's "brain" - billions of numbers that determine how it thinks</li>
                                    <li><strong>\(\nabla_\theta\):</strong> "Which direction should we adjust the brain to get better?"</li>
                                    <li><strong>\(\pi_\theta(a_t | s_t)\):</strong> "How likely is the model to choose token \(a_t\) given context \(s_t\)?"</li>
                                    <li><strong>\(\log \pi_\theta\):</strong> We use log because it makes math easier and more stable</li>
                                    <li><strong>\(\hat{A}_t\):</strong> "Was this choice better (+) or worse (-) than average?"</li>
                                    <li><strong>\(\hat{\mathbb{E}}_t\):</strong> Average over all the examples we've seen</li>
                                </ul>
                                
                                <p><strong>The Learning Rule (Simplified):</strong></p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ L^{PG}(\theta) = \hat{\mathbb{E}}_t [\log \pi_\theta(a_t | s_t) \hat{A}_t] $$
                                </div>
                                
                                <p><strong>Think of it like this:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>If \(\hat{A}_t = +2\):</strong> "This token choice was really good! Make it 2x more likely next time."</li>
                                    <li><strong>If \(\hat{A}_t = 0\):</strong> "This choice was average. Don't change anything."</li>
                                    <li><strong>If \(\hat{A}_t = -1.5\):</strong> "This was a bad choice. Make it less likely next time."</li>
                                </ul>
                                
                                <p><strong>Real Example:</strong> If the model generates "The sky is purple" (bad, \(\hat{A}_t = -2\)), we decrease the probability of "purple" after "sky is". If it generates "The sky is blue" (good, \(\hat{A}_t = +1.5\)), we increase the probability of "blue".</p>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Teaching Better Explanations</h6>
                                <strong>Scenario:</strong> Training ChatGPT to give better physics explanations<br><br>
                                <strong>User Prompt:</strong> "Explain quantum physics to a beginner"<br>
                                <strong>Current Context:</strong> "Quantum physics is"<br>
                                <strong>Token Choice:</strong> "fascinating" vs "complicated"<br><br>
                                <strong>Step 1: Current Policy Probabilities</strong><br>
                                • \(\pi_\theta(\text{"fascinating"}|\text{context}) = 0.7\) (70% chance)<br>
                                • \(\pi_\theta(\text{"complicated"}|\text{context}) = 0.3\) (30% chance)<br><br>
                                <strong>Step 2: Human Feedback Analysis</strong><br>
                                • Complete response with "fascinating": "Quantum physics is fascinating because it reveals how particles behave in surprising ways..."<br>
                                • Human rating: 4.5/5 (engaging, accessible)<br>
                                • Advantage: \(\hat{A}_t = +2.3\) (much better than average)<br><br>
                                <strong>Step 3: Policy Gradient Calculation</strong><br>
                                \(L^{PG} = \log(0.7) \times 2.3\)<br>
                                \(L^{PG} = -0.356 \times 2.3 = -0.82\)<br><br>
                                <strong>Step 4: Learning Effect</strong><br>
                                • Gradient pushes model to increase \(\pi_\theta(\text{"fascinating"})\) from 0.7 → 0.75<br>
                                • Model learns: "When explaining complex topics, use engaging language"<br>
                                • Future responses become more accessible and interesting<br><br>
                                <strong>Counter-example:</strong> If response was confusing (\(\hat{A}_t = -1.5\)), model would decrease probability of "complicated" language
                            </div>
                        </div>
                    </div>

                    <div class="workflow-card" data-step="2">
                        <h5>Trust Region Policy Optimization (TRPO): The Predecessor</h5>
                        <p class="card-explainer">PPO was designed to achieve TRPO's stability with a simpler first-order optimization approach. Understanding TRPO helps explain PPO's design choices.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>TRPO: Learning with Training Wheels</h6>
                                <p><strong>The Catastrophic Problem:</strong> Imagine you're learning to drive. If you make huge steering corrections, you'll crash! Similarly, if we make huge changes to an LLM's "brain," it might forget how to speak English entirely and start generating gibberish.</p>
                                
                                <p><strong>TRPO's Ingenious Solution:</strong> "Let's improve the model, but with safety guardrails!"</p>
                                
                                <p><strong>The Main Objective (What We Want to Maximize):</strong></p>
                                <div class="equation math-jax" style="font-size: 0.75em; padding: 6px; margin: 6px 0;">
                                    $$ \max_\theta \hat{\mathbb{E}}_t \left[ \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A}_t \right] $$
                                </div>
                                
                                <p><strong>Let's decode this step by step:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>\(\pi_\theta(a_t|s_t)\):</strong> New model's probability of choosing token \(a_t\)</li>
                                    <li><strong>\(\pi_{\theta_{old}}(a_t|s_t)\):</strong> Old model's probability of choosing the same token</li>
                                    <li><strong>The ratio \(\frac{\pi_\theta}{\pi_{\theta_{old}}}\):</strong> "How much more/less likely is this choice now?"</li>
                                    <li><strong>If ratio = 2:</strong> New model is 2x more likely to choose this token</li>
                                    <li><strong>If ratio = 0.5:</strong> New model is half as likely to choose this token</li>
                                    <li><strong>If ratio = 1:</strong> No change in probability</li>
                                </ul>
                                
                                <p><strong>The Logic:</strong> If \(\hat{A}_t > 0\) (good choice), we want ratio > 1. If \(\hat{A}_t < 0\) (bad choice), we want ratio < 1. Multiply them together and we get a score to maximize!</p>
                                
                                <p><strong>But Here's the Safety Constraint (The Training Wheels):</strong></p>
                                <div class="equation math-jax" style="font-size: 0.75em; padding: 6px; margin: 6px 0;">
                                    $$ \hat{\mathbb{E}}_t[KL[\pi_{\theta_{old}}(\cdot|s_t), \pi_\theta(\cdot|s_t)]] \leq \delta $$
                                </div>
                                
                                <p><strong>KL Divergence Explained Like You're 5:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>KL = 0:</strong> "The new model thinks exactly like the old model" (identical twins)</li>
                                    <li><strong>KL = 0.01:</strong> "Very similar thinking, just tiny differences" (siblings)</li>
                                    <li><strong>KL = 0.1:</strong> "Noticeably different but still related" (cousins)</li>
                                    <li><strong>KL = 1.0:</strong> "Very different thinking" (strangers)</li>
                                </ul>
                                
                                <p><strong>The Constraint Says:</strong> "You can improve the model, but don't let it become too different from what it was. Keep KL ≤ δ (usually 0.01)."</p>
                                
                                <p><strong>Why This is Hard:</strong> This creates a complex optimization problem that requires second-order derivatives (like computing the curvature of a mountain while climbing it). It's mathematically elegant but computationally expensive!</p>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Poetry Generation Constraints</h6>
                                <strong>Scenario:</strong> Training an LLM to write better poetry using TRPO<br><br>
                                <strong>User Prompt:</strong> "Write a haiku about artificial intelligence"<br><br>
                                <strong>Old Policy Response:</strong><br>
                                "Machines that can think<br>
                                Processing information<br>
                                Future is here now"<br>
                                • Probability: \(\pi_{old}(\text{this poem}|\text{prompt}) = 0.0023\)<br><br>
                                <strong>Proposed New Policy Response:</strong><br>
                                "Silicon neurons<br>
                                Dancing through digital dreams<br>
                                Consciousness blooms bright"<br>
                                • Probability: \(\pi_{new}(\text{this poem}|\text{prompt}) = 0.0089\)<br>
                                • Human rating: Much more creative and poetic<br><br>
                                <strong>TRPO Constraint Check:</strong><br>
                                • KL divergence limit: \(\delta = 0.01\) (stay close to old policy)<br>
                                • Actual KL: \(KL[\pi_{old}, \pi_{new}] = 0.023\) (too big a change!)<br>
                                • TRPO decision: Reject this update, it's too dramatic<br>
                                • Required action: Use smaller learning rate, make gradual improvements<br><br>
                                <strong>The Problem:</strong> TRPO's conservative approach means the model learns very slowly to be more creative, taking many small steps instead of one good leap<br><br>
                                <strong>PPO's Advantage:</strong> Clipping allows bigger improvements while staying stable
                            </div>
                        </div>
                    </div>

                    <div class="workflow-card" data-step="3">
                        <h5>PPO Core Idea: Clipped Surrogate Objective</h5>
                        <p class="card-explainer">PPO's main innovation is replacing TRPO's complex constrained optimization with a simple clipping mechanism that prevents destructive policy updates.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>PPO's Brilliant Solution: Smart Clipping</h6>
                                <p><strong>The Genius Insight:</strong> "What if instead of complex math constraints, we just put a simple 'speed limit' on how much the model can change?" This is PPO's revolutionary idea!</p>
                                
                                <p><strong>Step 1: Calculate the Change Ratio</strong></p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} $$
                                </div>
                                
                                <p><strong>This ratio tells us everything:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>r = 1.0:</strong> "No change - new model acts exactly like old model"</li>
                                    <li><strong>r = 1.5:</strong> "New model is 50% more likely to choose this token"</li>
                                    <li><strong>r = 0.7:</strong> "New model is 30% less likely to choose this token"</li>
                                    <li><strong>r = 3.0:</strong> "DANGER! New model is 3x more likely - too big a change!"</li>
                                    <li><strong>r = 0.1:</strong> "DANGER! New model almost never chooses this - too big a change!"</li>
                                </ul>
                                
                                <p><strong>Step 2: PPO's Clipping Magic</strong></p>
                                <div class="equation math-jax" style="font-size: 0.75em; padding: 6px; margin: 6px 0;">
                                    $$ L^{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[ \min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right) \right] $$
                                </div>
                                
                                <p><strong>The Clipping Function Explained:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>If \(\epsilon = 0.2\):</strong> We clip ratios to the range [0.8, 1.2]</li>
                                    <li><strong>If r = 0.5:</strong> clip(0.5, 0.8, 1.2) = 0.8 (bring it up to minimum)</li>
                                    <li><strong>If r = 1.0:</strong> clip(1.0, 0.8, 1.2) = 1.0 (no change needed)</li>
                                    <li><strong>If r = 2.5:</strong> clip(2.5, 0.8, 1.2) = 1.2 (bring it down to maximum)</li>
                                </ul>
                                
                                <p><strong>The "min" Operation - PPO's Safety Net:</strong></p>
                                <p>We compute TWO objectives: one with the original ratio, one with the clipped ratio. Then we take the minimum (most pessimistic) of the two.</p>
                                
                                <p><strong>Why This Works:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>If the model wants to make a good change (positive advantage):</strong> Clipping prevents it from getting too excited and making huge changes</li>
                                    <li><strong>If the model wants to make a bad change (negative advantage):</strong> Clipping prevents it from making huge destructive changes</li>
                                    <li><strong>The result:</strong> Steady, stable learning without catastrophic failures!</li>
                                </ul>
                                
                                <p><strong>Brilliant Simplicity:</strong> Instead of TRPO's complex second-order optimization, PPO just says "don't change by more than 20% in any direction" and it works amazingly well!</p>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Learning Programming Advice</h6>
                                <strong>Scenario:</strong> Training ChatGPT to give better programming advice<br><br>
                                <strong>User Question:</strong> "How do I learn programming effectively?"<br>
                                <strong>Context Generated So Far:</strong> "The best way to learn programming is"<br>
                                <strong>Token Decision:</strong> "practice" vs "reading"<br><br>
                                <strong>PPO Settings:</strong> \(\epsilon = 0.2\) (clipping range [0.8, 1.2])<br><br>
                                <strong>Case 1: High-Reward Token (Practice-Focused Response)</strong><br>
                                • Complete response: "The best way to learn programming is practice - build projects, solve coding challenges, and write code daily"<br>
                                • Human feedback: Very helpful, actionable advice<br>
                                • Advantage: \(\hat{A}_t = +1.8\) (much better than average)<br>
                                • Old policy: \(\pi_{old}(\text{"practice"}|\text{context}) = 0.4\) (40% chance)<br>
                                • New policy: \(\pi_{new}(\text{"practice"}|\text{context}) = 0.65\) (65% chance)<br>
                                • Ratio: \(r_t = 0.65/0.4 = 1.625\) (62.5% increase!)<br>
                                • Clipped ratio: \(\min(1.625, 1.2) = 1.2\) (capped at 20% increase)<br>
                                • Final objective: \(\min(1.625 \times 1.8, 1.2 \times 1.8) = \min(2.925, 2.16) = 2.16\)<br><br>
                                <strong>Case 2: Alternative Token (Reading-Focused)</strong><br>
                                • Complete response: "The best way to learn programming is reading documentation and tutorials thoroughly"<br>
                                • Human feedback: Somewhat helpful but less actionable<br>
                                • Advantage: \(\hat{A}_t = -0.9\) (below average)<br>
                                • Clipping prevents over-penalization of this reasonable alternative<br><br>
                                <strong>Learning Result:</strong> Model learns to favor hands-on, practical advice while not completely dismissing other approaches
                            </div>
                        </div>
                    </div>

                    <div class="workflow-card" data-step="4">
                        <h5>PPO Algorithm: High-Level Structure</h5>
                        <p class="card-explainer">PPO follows a simple iterative process: collect trajectories, compute advantages, and optimize the clipped surrogate objective for multiple epochs.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>PPO Algorithm: The Complete Recipe for Success</h6>
                                <p><strong>Think of PPO like a chef perfecting a recipe through careful experimentation:</strong></p>
                                
                                <p><strong>Phase 1: Setup Your Kitchen (Initialization)</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>\(\pi_{\theta_0}\):</strong> Your "chef" (the language model that generates responses)</li>
                                    <li><strong>\(V_{\phi_0}\):</strong> Your "food critic" (predicts how good a response will be)</li>
                                    <li><strong>Both start as apprentices and will get better together!</strong></li>
                                </ul>
                                
                                <p><strong>Phase 2: The Learning Cycle (Repeated Thousands of Times)</strong></p>
                                
                                <p><strong>Step 1: Practice Cooking (Data Collection)</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li>Give the model 512 different prompts (like "Explain gravity")</li>
                                    <li>Let it generate responses using current policy \(\pi_{\theta_k}\)</li>
                                    <li>This is expensive! Each response takes GPU time and energy</li>
                                    <li>Result: 512 prompt-response pairs to learn from</li>
                                </ul>
                                
                                <p><strong>Step 2: Get Expert Feedback (Reward & Advantage Computation)</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li>Feed each response to the reward model (the "expert judge")</li>
                                    <li>Get scores like: "This explanation is 8/10" or "This is confusing, 3/10"</li>
                                    <li>Compute advantages \(\hat{A}_t\): "Was this response better or worse than expected?"</li>
                                    <li>Positive advantage = "Surprisingly good!" Negative = "Disappointing"</li>
                                </ul>
                                
                                <p><strong>Step 3: Learn Intensively (The 4-Epoch Training)</strong></p>
                                <p><strong>Here's PPO's secret sauce:</strong> Instead of throwing away the data after one use, we learn from it 4 times!</p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Epoch 1:</strong> "Here's what we did and how good it was..."</li>
                                    <li><strong>Epoch 2:</strong> "Let me think about this again..."</li>
                                    <li><strong>Epoch 3:</strong> "I'm starting to see the patterns..."</li>
                                    <li><strong>Epoch 4:</strong> "Now I really understand!"</li>
                                    <li><strong>Clipping ensures:</strong> We don't "over-learn" and break the model</li>
                                </ul>
                                
                                <p><strong>Step 4: Graduation (Policy Update)</strong></p>
                                <p>After 4 epochs of learning, \(\theta_k\) becomes \(\theta_{k+1}\) - a smarter, better model!</p>
                                
                                <p><strong>The Economics:</strong> Data collection is expensive (like hiring a chef to cook), but learning from existing data is cheap (like studying recipes). PPO maximizes learning per dollar spent!</p>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Customer Service Training</h6>
                                <strong>Scenario:</strong> Training an AI assistant for customer service<br><br>
                                <strong>Training Batch Example:</strong><br>
                                • 512 customer complaints processed simultaneously<br>
                                • Each response up to 2048 tokens (full conversation)<br>
                                • Learning rate: 1e-6 (very careful updates)<br>
                                • Clipping: \(\epsilon = 0.2\) (20% maximum change)<br>
                                • KL penalty: \(\beta = 0.02\) (stay professional)<br><br>
                                <strong>Sample Customer Complaint:</strong><br>
                                "My order was delayed and I'm very frustrated. This is unacceptable!"<br><br>
                                <strong>Response A (High Reward):</strong><br>
                                "I completely understand your frustration, and I sincerely apologize for the delay. Let me check your order status and see how we can make this right..."<br>
                                • Empathetic, professional, solution-focused<br>
                                • Reward score: 4.2/5.0<br><br>
                                <strong>Response B (Low Reward):</strong><br>
                                "Orders sometimes get delayed. Please be patient."<br>
                                • Dismissive, unhelpful<br>
                                • Reward score: 1.1/5.0<br><br>
                                <strong>Training Effect:</strong><br>
                                • Model learns to prioritize empathy and problem-solving<br>
                                • Increases probability of helpful phrases like "I understand" and "Let me help"<br>
                                • Decreases probability of dismissive language<br>
                                • After 10,000 iterations: Customer satisfaction increases from 3.2/5 to 4.1/5
                            </div>
                        </div>
                    </div>

                    <div class="workflow-card" data-step="5">
                        <h5>The Complete PPO Objective Function</h5>
                        <p class="card-explainer">In practice, PPO combines the clipped policy loss with value function training and an entropy bonus to create a robust, multi-objective optimization.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>PPO's Triple Optimization: Juggling Three Goals</h6>
                                <p><strong>Imagine training a student who must:</strong> (1) Give better answers, (2) Predict test scores accurately, and (3) Stay creative. PPO does all three simultaneously!</p>
                                
                                <div class="equation math-jax" style="font-size: 0.75em; padding: 6px; margin: 6px 0;">
                                    $$ L_t(\theta, \phi) = L_t^{CLIP}(\theta) - c_1 L_t^{VF}(\phi) + c_2 S[\pi_\theta](s_t) $$
                                </div>
                                
                                <p><strong>🎯 Goal 1: Better Token Choices (Policy Loss)</strong></p>
                                <p>\(L_t^{CLIP}(\theta)\) = "Make the model choose better words/tokens"</p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li>This is the clipped objective we learned about</li>
                                    <li>Positive when model improves, negative when it gets worse</li>
                                    <li>The main driver of better responses</li>
                                </ul>
                                
                                <p><strong>🔮 Goal 2: Better Predictions (Value Function Loss)</strong></p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ L_t^{VF}(\phi) = (V_\phi(s_t) - V_t^{targ})^2 $$
                                </div>
                                
                                <p><strong>The Value Function is Like a Fortune Teller:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>\(V_\phi(s_t)\):</strong> "I predict this response will get a reward of 2.3"</li>
                                    <li><strong>\(V_t^{targ}\):</strong> "Actually, it got a reward of 2.8"</li>
                                    <li><strong>Error:</strong> (2.3 - 2.8)² = 0.25</li>
                                    <li><strong>Goal:</strong> Make the fortune teller more accurate over time</li>
                                    <li><strong>Why we need this:</strong> Good predictions → better advantage estimates → better learning</li>
                                </ul>
                                
                                <p><strong>🎨 Goal 3: Stay Creative (Entropy Bonus)</strong></p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ S[\pi_\theta](s_t) = -\sum_a \pi_\theta(a|s_t) \log \pi_\theta(a|s_t) $$
                                </div>
                                
                                <p><strong>Entropy Explained with a Coin Flip Analogy:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Fair coin (50/50):</strong> High entropy = maximum uncertainty/creativity</li>
                                    <li><strong>Biased coin (99/1):</strong> Low entropy = very predictable/boring</li>
                                    <li><strong>For LLMs:</strong> High entropy = considers many word choices</li>
                                    <li><strong>Low entropy problem:</strong> Model always says "The answer is..." (repetitive!)</li>
                                    <li><strong>Solution:</strong> Small bonus for staying diverse and creative</li>
                                </ul>
                                
                                <p><strong>⚖️ Balancing Act:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>\(c_1 = 0.5\):</strong> Value function training is half as important as policy</li>
                                    <li><strong>\(c_2 = 0.01\):</strong> Entropy gets a tiny bonus (don't want too much randomness)</li>
                                    <li><strong>The art:</strong> Tuning these coefficients for optimal performance</li>
                                </ul>
                                
                                <p><strong>🧠 Why This Works:</strong> Like teaching a student to be accurate AND creative AND self-aware, PPO creates well-rounded AI that doesn't just memorize but truly understands!</p>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Teaching Children About AI</h6>
                                <strong>Scenario:</strong> Training ChatGPT to explain complex topics to children<br><br>
                                <strong>Parent's Request:</strong> "Explain machine learning to my 5-year-old"<br><br>
                                <strong>Generated Response:</strong><br>
                                "Machine learning is like teaching a computer to recognize patterns, just like how you learn to recognize different animals. When you see a dog, you know it's a dog because you've seen many dogs before and learned what makes them special - four legs, fur, wagging tail. Computers can learn the same way!"<br><br>
                                <strong>Step 1: Reward Model Evaluation</strong><br>
                                • Age-appropriateness: 4.5/5 (uses simple analogies)<br>
                                • Accuracy: 4.2/5 (correct but simplified)<br>
                                • Engagement: 4.0/5 (relatable examples)<br>
                                • Overall reward: \(r = +2.1\) (much better than average)<br><br>
                                <strong>Step 2: PPO Loss Components</strong><br>
                                • Clipped policy loss: \(L^{CLIP} = -0.73\) (encourages this response)<br>
                                • Value function prediction: \(V = 1.8\) ("I think this will get ~1.8 reward")<br>
                                • Actual target: \(V^{targ} = 2.1\) (it got 2.1, better than predicted!)<br>
                                • Value loss: \(L^{VF} = (1.8-2.1)^2 = 0.09\) (improve prediction)<br>
                                • Entropy: \(S = 3.2\) (good word variety, not repetitive)<br><br>
                                <strong>Step 3: Combined Optimization</strong><br>
                                \(L = -0.73 - 0.5 \times 0.09 + 0.01 \times 3.2 = -0.683\)<br><br>
                                <strong>Learning Outcome:</strong><br>
                                • Model learns to use animal analogies for children<br>
                                • Value function learns that simple explanations get higher rewards<br>
                                • Future responses become more child-friendly and engaging
                            </div>
                        </div>
                    </div>

                    <div class="workflow-card" data-step="6">
                        <h5>Generalized Advantage Estimation (GAE)</h5>
                        <p class="card-explainer">The quality of advantage estimation is critical for PPO performance. GAE provides a sophisticated method to balance bias and variance in advantage estimates.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>GAE: Computing "How Good Was This Choice?"</h6>
                                <p><strong>Step 1: Temporal Difference (TD) Error</strong></p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) $$
                                </div>
                                <p><strong>Intuition:</strong> Did we get more reward than expected? \(r_t\) = actual reward, \(V(s_t)\) = what we expected, \(\gamma V(s_{t+1})\) = discounted future value.</p>
                                <p><strong>Step 2: GAE Advantage (Weighted Average of TD Errors)</strong></p>
                                <div class="equation math-jax" style="font-size: 0.75em; padding: 6px; margin: 6px 0;">
                                    $$ \hat{A}_t^{GAE(\gamma, \lambda)} = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l} $$
                                </div>
                                <p><strong>What this does:</strong> Look at TD error now (\(\delta_t\)) and future TD errors (\(\delta_{t+1}, \delta_{t+2}...\)), but weight future errors less (\(\gamma\lambda < 1\)).</p>
                                <p><strong>Practical Recursive Form:</strong></p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ \hat{A}_t = \delta_t + \gamma\lambda \hat{A}_{t+1} $$
                                </div>
                                <p><strong>\(\lambda\) parameter:</strong> \(\lambda = 0\) = only use immediate TD error. \(\lambda = 1\) = use all future TD errors equally. \(\lambda = 0.95\) = good balance.</p>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Poetry Generation Analysis</h6>
                                <strong>Scenario:</strong> Training an LLM to write better poetry<br><br>
                                <strong>User Request:</strong> "Write a haiku about artificial intelligence"<br><br>
                                <strong>Generated Haiku (token by token):</strong><br>
                                "Silicon minds dream<br>
                                Processing data streams<br>
                                Consciousness awakening"<br><br>
                                <strong>Key Tokens Analyzed:</strong> ["Silicon", "minds", "awakening"]<br>
                                • \(\gamma = 0.99\) (future rewards matter), \(\lambda = 0.95\) (balance bias/variance)<br><br>
                                <strong>Step 1: Immediate Rewards</strong><br>
                                • "Silicon": 0.5 (technical but cold)<br>
                                • "minds": 1.2 (adds humanity)<br>
                                • "awakening": 2.1 (powerful, evocative ending)<br><br>
                                <strong>Step 2: Value Function Predictions</strong><br>
                                • After "Silicon": \(V = 0.8\) ("this poem might be okay")<br>
                                • After "minds": \(V = 1.5\) ("getting better, more engaging")<br>
                                • After "awakening": \(V = 1.9\) ("strong finish expected")<br><br>
                                <strong>Step 3: TD Error Calculations</strong><br>
                                • \(\delta_0 = 0.5 + 0.99 \times 1.5 - 0.8 = 1.185\) (better than expected)<br>
                                • \(\delta_1 = 1.2 + 0.99 \times 1.9 - 1.5 = 1.581\) (much better)<br>
                                • \(\delta_2 = 2.1 + 0 - 1.9 = 0.2\) (slightly better than predicted)<br><br>
                                <strong>Step 4: GAE Advantages (Credit Assignment)</strong><br>
                                • \(\hat{A}_2 = 0.2\) ("awakening" was good)<br>
                                • \(\hat{A}_1 = 1.581 + 0.99 \times 0.95 \times 0.2 = 1.769\) ("minds" set up the great ending)<br>
                                • \(\hat{A}_0 = 1.185 + 0.99 \times 0.95 \times 1.769 = 2.836\) ("Silicon" started a winning poem)<br><br>
                                <strong>Learning Outcome:</strong> Model learns that technical terms can work if they lead to emotional payoffs
                            </div>
                        </div>
                    </div>

                    <div class="workflow-card" data-step="7">
                        <h5>PPO Training Loop: Implementation Details</h5>
                        <p class="card-explainer">The PPO training loop efficiently implements the clipped objective with multiple epochs and minibatch updates to maximize sample efficiency.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>PPO Training Loop: The Full Recipe</h6>
                                <p><strong>Think of it like learning to cook from practice:</strong></p>
                                <ul style="font-size: 0.8em; margin: 8px 0;">
                                    <li><strong>1. Practice:</strong> Generate responses to prompts (collect data)</li>
                                    <li><strong>2. Get feedback:</strong> Reward model scores responses, compute advantages</li>
                                    <li><strong>3. Learn (4 times from same practice session):</strong></li>
                                    <li>&nbsp;&nbsp;• <strong>Shuffle:</strong> Mix up the data randomly</li>
                                    <li>&nbsp;&nbsp;• <strong>Small bites:</strong> Process data in small batches (64-512 examples)</li>
                                    <li>&nbsp;&nbsp;• <strong>Improve policy:</strong> Update model weights using clipped objective</li>
                                    <li>&nbsp;&nbsp;• <strong>Improve critic:</strong> Train value function to predict rewards better</li>
                                </ul>
                                <p><strong>Why repeat 4 times?</strong> Generating data is expensive (requires running the LLM), so we squeeze maximum learning from each batch. Clipping prevents us from "over-learning" and ruining the model.</p>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Medical Q&A Training Progress</h6>
                                <strong>Scenario:</strong> Training an AI medical assistant over 10,000 training steps<br><br>
                                <strong>Sample Question:</strong> "What should I do about persistent headaches?"<br><br>
                                <strong>Early Training Response (Step 100):</strong><br>
                                "Headaches can be caused by many things. Try drinking water."<br>
                                • Reward: 0.2/5.0 (too brief, not helpful)<br>
                                • Length: 45 tokens (too short)<br><br>
                                <strong>Late Training Response (Step 10,000):</strong><br>
                                "Persistent headaches can have various causes including dehydration, stress, or underlying conditions. I recommend: 1) Keep a headache diary to track triggers, 2) Ensure adequate hydration and sleep, 3) Consider stress management techniques, and 4) Consult a healthcare provider if headaches persist or worsen, especially if accompanied by other symptoms."<br>
                                • Reward: 1.8/5.0 (comprehensive, actionable)<br>
                                • Length: 120 tokens (appropriately detailed)<br><br>
                                <strong>Training Metrics Evolution:</strong><br>
                                • Policy loss: -0.05 → -0.23 (learning to maximize helpful responses)<br>
                                • Value loss: 0.8 → 0.3 (better at predicting response quality)<br>
                                • KL penalty: 0.02 (staying close to medical training base)<br>
                                • Clip fraction: 0.18 (healthy learning rate)<br><br>
                                <strong>Quality Improvements:</strong><br>
                                • Helpfulness: 3.2/5 → 4.1/5 (more actionable advice)<br>
                                • Harmlessness: 4.8/5 → 4.9/5 (appropriate medical disclaimers)<br>
                                • Honesty: 3.9/5 → 4.3/5 (acknowledges limitations)<br><br>
                                <strong>Safety Check:</strong> If KL > 0.05, model might start giving inappropriate medical advice
                            </div>
                        </div>
                    </div>

                    <div class="workflow-card" data-step="8">
                        <h5>PPO Variants and Practical Considerations</h5>
                        <p class="card-explainer">PPO has several variants and important implementation considerations that affect its performance in practice.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>PPO Alternative: Adaptive KL Penalty</h6>
                                <p><strong>Instead of clipping, use a "speed limit" that adapts:</strong></p>
                                <div class="equation math-jax" style="font-size: 0.75em; padding: 6px; margin: 6px 0;">
                                    $$ L^{KLPEN}(\theta) = \hat{\mathbb{E}}_t[\log \pi_\theta(a_t | s_t) \hat{A}_t - \beta KL[\pi_{\theta_{old}}, \pi_\theta]] $$
                                </div>
                                <p><strong>How it works:</strong> Maximize reward (first term) but subtract a penalty for changing too much (second term). β controls how strict the penalty is.</p>
                                <p><strong>Adaptive β (Smart Speed Limit):</strong></p>
                                <ul style="font-size: 0.8em; margin: 6px 0;">
                                    <li><strong>Going too fast?</strong> If KL > 1.5 × target: β ← 2β (stricter penalty)</li>
                                    <li><strong>Going too slow?</strong> If KL < target/1.5: β ← β/2 (relax penalty)</li>
                                </ul>
                                <p><strong>Common Failure Modes:</strong> Reward hacking (model finds loopholes), distribution shift (model forgets original training), training instability</p>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Creative Writing Assistant Tuning</h6>
                                <strong>Scenario:</strong> Fine-tuning hyperparameters for a creative writing assistant<br><br>
                                <strong>Test Prompt:</strong> "Write an engaging opening paragraph for a mystery novel"<br><br>
                                <strong>KL Penalty β Experiments:</strong><br><br>
                                <strong>β = 0.001 (Too Permissive):</strong><br>
                                "The dark mysterious shadow crept through the dark mysterious alley of mysterious darkness with mysterious intent..."<br>
                                • Result: Repetitive, incoherent after 1K steps<br>
                                • Problem: No constraint on staying reasonable<br><br>
                                <strong>β = 0.02 (Just Right):</strong><br>
                                "Detective Sarah Chen had seen enough crime scenes to know when something didn't add up, but the pristine living room with a single drop of blood on the white sofa told a story that made her stomach turn."<br>
                                • Result: 4.1/5 human preference, engaging and coherent<br>
                                • Sweet spot: Creative but grounded<br><br>
                                <strong>β = 0.1 (Too Conservative):</strong><br>
                                "There was a crime. The detective investigated. It was mysterious."<br>
                                • Result: 3.2/5 preference (boring, generic)<br>
                                • Problem: Too afraid to deviate from training data<br><br>
                                <strong>Learning Rate Experiments:</strong><br>
                                • 5e-6: Fast improvement but unstable - starts writing nonsense<br>
                                • 1e-6: Gradual, stable improvement in creativity<br>
                                • 5e-7: Too slow, still generic after 20K steps<br><br>
                                <strong>Reward Hacking Example:</strong> When β too low, model learns to repeat high-scoring phrases like "compelling narrative" and "riveting mystery" regardless of context
                            </div>
                        </div>
                    </div>
                </div>
                <div class="card-navigation">
                    <button id="ppo-prev" disabled>Previous</button>
                    <div class="step-indicators">
                        <span class="dot active" data-step="1"></span>
                        <span class="dot" data-step="2"></span>
                        <span class="dot" data-step="3"></span>
                        <span class="dot" data-step="4"></span>
                        <span class="dot" data-step="5"></span>
                        <span class="dot" data-step="6"></span>
                        <span class="dot" data-step="7"></span>
                        <span class="dot" data-step="8"></span>
                    </div>
                    <button id="ppo-next">Next</button>
                </div>
            </div>

            <div class="two-col-layout" style="margin-top: 30px;">
                <div class="col">
                    <h4>Key Takeaways</h4>
                    <div class="example-box" style="margin-top:0; background-color: #fff;">
                        <ul>
                            <li><strong>SFT</strong> creates a solid foundation by teaching the model good examples</li>
                            <li><strong>Reward Modeling</strong> learns human preferences from comparative rankings</li>
                            <li><strong>PPO</strong> optimizes the policy to maximize rewards while staying stable</li>
                        </ul>
                        <p><strong>Why this works:</strong> Each stage builds on the previous one, creating a robust alignment pipeline that scales human feedback effectively.</p>
                    </div>
                </div>
                <div class="col">
                    <h4>Implementation Notes</h4>
                    <div class="example-box" style="margin-top:0; background-color: #fff;">
                        <ul>
                            <li><strong>Data Requirements:</strong> SFT needs ~10K examples, RM needs ~50K preference pairs</li>
                            <li><strong>Computational Cost:</strong> RL stage is most expensive (requires multiple model copies)</li>
                            <li><strong>Hyperparameters:</strong> \(\beta\) controls exploration vs. exploitation trade-off</li>
                        </ul>
                        <p><strong>Common Issues:</strong> Reward hacking, distribution shift, and instability during RL training.</p>
                    </div>
                </div>
            </div>
        </div>

        <div class="container">
            <h2>2. A More Direct Path: Direct Preference Optimization (DPO)</h2>
            <p>Training a separate reward model can be complex. <strong>Direct Preference Optimization (DPO)</strong> bypasses it by leveraging a mathematical relationship to directly optimize the policy on preference data. Below is an interactive walkthrough of DPO's key innovations and mathematical foundations.</p>
            
            <h3>PPO vs. DPO at a Glance</h3>
            <div class="workflow-container-2col">
                <div class="workflow-col">
                    <h4>PPO (Explicit Reward)</h4>
                    <div class="workflow-mini">
                        <div class="workflow-step">Preference Data</div>
                        <div class="workflow-arrow-down">&darr;</div>
                        <div class="workflow-step highlight1">Train Reward Model</div>
                        <div class="workflow-arrow-down">&darr;</div>
                        <div class="workflow-step highlight2">RL Fine-tuning</div>
                        <div class="workflow-arrow-down">&darr;</div>
                        <div class="workflow-step final">Aligned LLM</div>
                    </div>
                </div>
                <div class="workflow-col">
                    <h4>DPO (Implicit Reward)</h4>
                    <div class="workflow-mini">
                        <div class="workflow-step">Preference Data</div>
                        <div class="workflow-arrow-down">&darr;</div>
                        <div class="workflow-step highlight3">Directly Optimize Policy<br/>via DPO Loss</div>
                        <div class="workflow-arrow-down">&darr;</div>
                        <div class="workflow-step final">Aligned LLM</div>
                    </div>
                </div>
            </div>

            <div id="dpo-interactive-workflow">
                <div class="card-container">
                    <div class="workflow-card active" data-step="1">
                        <h5>The Bradley-Terry Preference Model</h5>
                        <p class="card-explainer">DPO builds on the Bradley-Terry model, which converts human preferences into mathematical probabilities that can be optimized.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>Bradley-Terry: The Foundation of Preference Learning</h6>
                                <p><strong>The Big Picture:</strong> Imagine you're a judge in a cooking competition. When you taste two dishes, you prefer one over the other. But how do we turn that preference into math? That's what Bradley-Terry does!</p>
                                
                                <p><strong>The Core Assumption:</strong> Every response has a hidden "quality score" \(r^*(x,y)\) that we can't see directly.</p>
                                
                                <p><strong>The Mathematical Heart:</strong></p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ p(y_w \succ y_l | x) = \frac{\exp(r^*(x, y_w))}{\exp(r^*(x, y_w)) + \exp(r^*(x, y_l))} = \sigma(r^*(x, y_w) - r^*(x, y_l)) $$
                                </div>
                                
                                <p><strong>Let's break this down piece by piece:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>\(r^*(x, y)\):</strong> The true, hidden quality of response \(y\) to prompt \(x\)</li>
                                    <li><strong>\(\exp(r^*)\):</strong> Convert scores to positive numbers (like turning grades into probabilities)</li>
                                    <li><strong>Softmax:</strong> \(\frac{\exp(r_w)}{\exp(r_w) + \exp(r_l)}\) normalizes so probabilities sum to 1</li>
                                    <li><strong>\(\sigma(r_w - r_l)\):</strong> Sigmoid of the difference - elegant shorthand!</li>
                                </ul>
                                
                                <p><strong>The Intuition:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>If \(r_w - r_l = +3\):</strong> Winner is much better → \(\sigma(3) = 95\%\) chance of preference</li>
                                    <li><strong>If \(r_w - r_l = 0\):</strong> Both equally good → \(\sigma(0) = 50\%\) chance (coin flip)</li>
                                    <li><strong>If \(r_w - r_l = -2\):</strong> Winner is actually worse → \(\sigma(-2) = 12\%\) chance</li>
                                </ul>
                                
                                <p><strong>The Magic:</strong> We don't need to know the exact scores \(r^*\) - we only need their difference! This is why preference learning works so well.</p>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Science Education Assistant</h6>
                                <strong>Scenario:</strong> Training an AI tutor to explain scientific concepts<br><br>
                                <strong>Student Question:</strong> "Can you explain how photosynthesis works?"<br><br>
                                <strong>Response A (Winner - Detailed & Engaging):</strong><br>
                                "Photosynthesis is like a plant's kitchen! Plants use chlorophyll (the green stuff in leaves) as their cooking equipment to combine sunlight, water from roots, and carbon dioxide from air. The 'recipe' produces glucose (plant food) and oxygen (which we breathe). The chemical equation is: 6CO₂ + 6H₂O + light energy → C₆H₁₂O₆ + 6O₂. This process happens in tiny structures called chloroplasts and is why plants are green and why we have oxygen to breathe!"<br>
                                • Educational value: High (uses analogy + science)<br>
                                • Engagement: High (relatable examples)<br>
                                • Accuracy: High (correct equation and facts)<br><br>
                                <strong>Response B (Loser - Oversimplified):</strong><br>
                                "Plants eat sunlight and make food."<br>
                                • Educational value: Low (no real explanation)<br>
                                • Engagement: Low (boring, unhelpful)<br>
                                • Accuracy: Misleading (plants don't "eat" sunlight)<br><br>
                                <strong>Hidden Quality Assessment:</strong><br>
                                • \(r^*(x, y_w) = 2.5\) (comprehensive, accurate, engaging)<br>
                                • \(r^*(x, y_l) = -1.0\) (unhelpful, potentially confusing)<br><br>
                                <strong>Bradley-Terry Preference Calculation:</strong><br>
                                • Score difference: \(2.5 - (-1.0) = 3.5\)<br>
                                • Preference probability: \(\sigma(3.5) = \frac{1}{1+e^{-3.5}} = 97.1\%\)<br><br>
                                <strong>Learning Insight:</strong> 97.1% of humans would prefer the detailed explanation, so DPO will strongly encourage similar comprehensive responses
                            </div>
                        </div>
                    </div>

                    <div class="workflow-card" data-step="2">
                        <h5>Traditional RLHF: The Three-Stage Pipeline</h5>
                        <p class="card-explainer">Before DPO, alignment required three complex stages. Understanding this helps appreciate DPO's elegant simplification.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>RLHF: The Traditional Way (Complex but Effective)</h6>
                                <p><strong>Stage 1: Supervised Fine-Tuning (SFT)</strong></p>
                                <p>Start with a base model and teach it to follow instructions using high-quality examples. This creates our reference policy \(\pi_{\text{ref}}\).</p>
                                
                                <p><strong>Stage 2: Reward Modeling</strong></p>
                                <p>Train a separate neural network \(r_\phi(x,y)\) to predict human preferences using the Bradley-Terry model:</p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ \mathcal{L}_{R}(r_\phi) = -\mathbb{E}_{(x,y_w,y_l)} \left[ \log \sigma(r_\phi(x, y_w) - r_\phi(x, y_l)) \right] $$
                                </div>
                                <p>This is a binary classification problem: "Is response A better than response B?"</p>
                                
                                <p><strong>Stage 3: RL Fine-Tuning (The Hard Part)</strong></p>
                                <p>Use reinforcement learning to optimize the policy using the reward model:</p>
                                <div class="equation math-jax" style="font-size: 0.75em; padding: 6px; margin: 6px 0;">
                                    $$ \max_{\pi_\theta} \mathbb{E}_{y \sim \pi_\theta(y|x)} [r_\phi(x,y)] - \beta \mathbb{D}_{KL}(\pi_\theta(y|x) || \pi_{\text{ref}}(y|x)) $$
                                </div>
                                
                                <p><strong>Why This is Hard:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Three separate models:</strong> Base model, reward model, and policy model</li>
                                    <li><strong>Complex RL algorithms:</strong> PPO, TRPO, or other policy gradient methods</li>
                                    <li><strong>Instability:</strong> RL training can be unstable and sensitive to hyperparameters</li>
                                    <li><strong>Memory intensive:</strong> Need to store multiple large models simultaneously</li>
                                    <li><strong>Reward hacking:</strong> Model might find ways to "cheat" the reward function</li>
                                </ul>
                                
                                <p><strong>The KL Term Explained:</strong> \(\beta \mathbb{D}_{KL}\) prevents the model from drifting too far from the original SFT model, maintaining coherent language generation.</p>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Traditional RLHF Challenges</h6>
                                <strong>Scenario:</strong> Training a customer service chatbot using traditional RLHF<br><br>
                                <strong>The Three-Model Complexity:</strong><br>
                                1. <strong>Base Model:</strong> General language understanding<br>
                                2. <strong>SFT Model:</strong> Learns customer service examples<br>
                                3. <strong>Reward Model:</strong> Learns to score responses like humans<br>
                                4. <strong>Policy Model:</strong> Final optimized chatbot<br><br>
                                <strong>Training Timeline & Challenges:</strong><br><br>
                                <strong>Stage 1 - SFT (3 days):</strong><br>
                                • Teach basic customer service responses<br>
                                • Example: "Thank you for contacting us. How can I help?"<br>
                                • Challenge: Limited to training examples only<br><br>
                                <strong>Stage 2 - Reward Modeling (1 day):</strong><br>
                                • Train separate model to score responses<br>
                                • Example: "I understand your frustration" = 4.2/5<br>
                                • Challenge: Reward model can overfit to training preferences<br><br>
                                <strong>Stage 3 - PPO Training (5 days):</strong><br>
                                • Use reward model to improve policy<br>
                                • Challenge: PPO is sensitive to hyperparameters<br>
                                • Common failure: Model starts generating gibberish to "hack" rewards<br><br>
                                <strong>Real Problems Encountered:</strong><br>
                                • Reward model gives high scores to nonsensical responses<br>
                                • Policy collapse: "Thank you thank you thank you..." (repetitive)<br>
                                • Hyperparameter sensitivity: Small changes break everything<br>
                                • Complex pipeline: Three separate training processes to manage
                            </div>
                        </div>
                    </div>

                    <div class="workflow-card" data-step="3">
                        <h5>DPO's Key Insight: The Optimal Policy-Reward Relationship</h5>
                        <p class="card-explainer">DPO's breakthrough comes from a mathematical relationship that allows us to express rewards directly in terms of policies.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>The Mathematical Breakthrough: Eliminating the Reward Model</h6>
                                <p><strong>Step 1: The Optimal Policy Formula</strong></p>
                                <p>For any reward function \(r(x,y)\), the optimal policy that maximizes the RLHF objective has this exact form:</p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ \pi_r(y|x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y|x) \exp\left(\frac{1}{\beta}r(x,y)\right) $$
                                </div>
                                
                                <p><strong>What each part means:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>\(\pi_r(y|x)\):</strong> The optimal policy for reward function \(r\)</li>
                                    <li><strong>\(\pi_{\text{ref}}(y|x)\):</strong> The reference policy (our starting point)</li>
                                    <li><strong>\(\exp(\frac{1}{\beta}r(x,y))\):</strong> Exponential of scaled reward (higher reward = higher probability)</li>
                                    <li><strong>\(Z(x)\):</strong> Normalization constant (ensures probabilities sum to 1)</li>
                                </ul>
                                
                                <p><strong>Step 2: The Brilliant Inversion</strong></p>
                                <p>Instead of finding the optimal policy for a given reward, we can solve for the reward given a policy:</p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ r(x,y) = \beta \log\left(\frac{\pi_r(y|x)}{\pi_{\text{ref}}(y|x)}\right) + \beta \log(Z(x)) $$
                                </div>
                                
                                <p><strong>The Magic Cancellation:</strong></p>
                                <p>When we compute the reward difference for two responses, the \(Z(x)\) terms cancel out perfectly:</p>
                                <div class="equation math-jax" style="font-size: 0.75em; padding: 6px; margin: 6px 0;">
                                    $$ r(x, y_w) - r(x, y_l) = \beta \left( \log\frac{\pi(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \log\frac{\pi(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) $$
                                </div>
                                
                                <p><strong>This is revolutionary because:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li>We can compute reward differences using only policy probabilities</li>
                                    <li>No need for a separate reward model</li>
                                    <li>The intractable partition function \(Z(x)\) disappears</li>
                                    <li>Direct optimization becomes possible</li>
                                </ul>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Poetry Assistant Breakthrough</h6>
                                <strong>Scenario:</strong> Training an AI poetry assistant using DPO's implicit rewards<br><br>
                                <strong>User Request:</strong> "Write a poem about artificial intelligence"<br><br>
                                <strong>Good Poem (Winner):</strong><br>
                                "Silicon dreams awaken,<br>
                                Algorithms dance with thought,<br>
                                Mind meets machine at last."<br>
                                • Creative, evocative, follows haiku structure<br>
                                • Current policy: \(\pi_{\theta}(y_w|x) = 0.15\) (15% chance)<br>
                                • Reference policy: \(\pi_{\text{ref}}(y_w|x) = 0.10\) (10% chance)<br><br>
                                <strong>Bad Poem (Loser):</strong><br>
                                "AI is good technology,<br>
                                Computers are smart,<br>
                                The end."<br>
                                • Boring, prosaic, not really poetry<br>
                                • Current policy: \(\pi_{\theta}(y_l|x) = 0.05\) (5% chance)<br>
                                • Reference policy: \(\pi_{\text{ref}}(y_l|x) = 0.08\) (8% chance)<br><br>
                                <strong>DPO's Magic: Implicit Reward Calculation</strong><br>
                                • \(\beta = 0.1\) (temperature parameter)<br>
                                • Good poem log ratio: \(\log(0.15/0.10) = 0.405\)<br>
                                • Bad poem log ratio: \(\log(0.05/0.08) = -0.470\)<br>
                                • Implicit reward difference: \(0.1 \times (0.405 - (-0.470)) = 0.0875\)<br><br>
                                <strong>The Breakthrough:</strong> Without ever training a reward model, DPO automatically knows the current policy values the good poem 0.0875 points higher than the bad poem. This drives learning toward more creative, poetic responses!
                            </div>
                        </div>
                    </div>

                    <div class="workflow-card" data-step="4">
                        <h5>The DPO Loss Function: Putting It All Together</h5>
                        <p class="card-explainer">By substituting the policy-based reward into the Bradley-Terry model, we get a simple classification loss that can be optimized directly.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>DPO Loss: The Final Elegant Formula</h6>
                                <p><strong>Starting from Bradley-Terry:</strong> We know human preferences follow:</p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ p(y_w \succ y_l | x) = \sigma(r(x, y_w) - r(x, y_l)) $$
                                </div>
                                
                                <p><strong>Substituting our policy-based reward:</strong></p>
                                <div class="equation math-jax" style="font-size: 0.75em; padding: 6px; margin: 6px 0;">
                                    $$ p(y_w \succ y_l | x) = \sigma\left(\beta \log\frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log\frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\right) $$
                                </div>
                                
                                <p><strong>The DPO Loss Function:</strong></p>
                                <div class="equation math-jax" style="font-size: 0.75em; padding: 6px; margin: 6px 0;">
                                    $$ \mathcal{L}_{\text{DPO}}(\pi_\theta) = -\mathbb{E}_{(x,y_w,y_l)}\left[\log \sigma\left(\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\right)\right] $$
                                </div>
                                
                                <p><strong>What this loss does:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Increases \(\pi_\theta(y_w|x)\):</strong> Make winning responses more likely</li>
                                    <li><strong>Decreases \(\pi_\theta(y_l|x)\):</strong> Make losing responses less likely</li>
                                    <li><strong>Relative to \(\pi_{\text{ref}}\):</strong> Stay close to the reference policy</li>
                                    <li><strong>Automatic weighting:</strong> Focus more on examples where the model is wrong</li>
                                </ul>
                                
                                <p><strong>Why this is beautiful:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li>✅ <strong>Simple:</strong> Just a classification loss, no RL needed</li>
                                    <li>✅ <strong>Stable:</strong> No complex RL dynamics or hyperparameter sensitivity</li>
                                    <li>✅ <strong>Memory efficient:</strong> Only need two models (policy + reference)</li>
                                    <li>✅ <strong>Theoretically grounded:</strong> Provably optimal under Bradley-Terry assumptions</li>
                                    <li>✅ <strong>Direct:</strong> Optimize exactly what you want (human preferences)</li>
                                </ul>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Complete DPO Training Step</h6>
                                <strong>Scenario:</strong> Training an AI tutor to explain machine learning concepts<br><br>
                                <strong>Student Question:</strong> "Can you explain what machine learning is?"<br><br>
                                <strong>Winner Response (Detailed & Clear):</strong><br>
                                "Machine learning is like teaching a computer to recognize patterns by showing it lots of examples. Instead of programming specific rules, we let the computer figure out patterns on its own. For example, to recognize cats in photos, we show it thousands of cat pictures and it learns what features make a cat - whiskers, pointy ears, etc."<br>
                                • \(\log \pi_{\text{ref}}(y_w|x) = -1.2\) (reference model probability)<br>
                                • \(\log \pi_{\theta}(y_w|x) = -1.0\) (current model is more likely to generate this)<br><br>
                                <strong>Loser Response (Vague & Unhelpful):</strong><br>
                                "Machine learning is when computers learn things automatically."<br>
                                • \(\log \pi_{\text{ref}}(y_l|x) = -1.5\) (reference model probability)<br>
                                • \(\log \pi_{\theta}(y_l|x) = -2.0\) (current model is less likely to generate this)<br><br>
                                <strong>DPO Loss Calculation (\(\beta = 0.1\)):</strong><br>
                                1. <strong>Log ratio differences:</strong><br>
                                   • Winner: \((-1.0) - (-1.2) = +0.2\) (model improved on good response)<br>
                                   • Loser: \((-2.0) - (-1.5) = -0.5\) (model worsened on bad response)<br>
                                2. <strong>Combined difference:</strong> \(0.1 \times (0.2 - (-0.5)) = 0.07\)<br>
                                3. <strong>Preference probability:</strong> \(\sigma(0.07) = 0.517\) (51.7% confident in ranking)<br>
                                4. <strong>Loss:</strong> \(-\log(0.517) = 0.66\)<br><br>
                                <strong>Learning Effect:</strong> The model will increase probability of detailed explanations and decrease probability of vague responses
                            </div>
                        </div>
                    </div>

                    <div class="workflow-card" data-step="5">
                        <h5>DPO Gradient: Understanding the Learning Dynamics</h5>
                        <p class="card-explainer">The DPO gradient has an elegant form that automatically focuses on misranked examples and provides stable learning.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>DPO Gradient: How the Model Actually Learns</h6>
                                <p><strong>The Gradient Formula:</strong></p>
                                <div class="equation math-jax" style="font-size: 0.75em; padding: 6px; margin: 6px 0;">
                                    $$ \nabla_\theta \mathcal{L}_{\text{DPO}} \propto -\beta \sigma(\hat{r}_\theta(y_l) - \hat{r}_\theta(y_w)) \left[ \nabla_\theta \log\pi_\theta(y_w|x) - \nabla_\theta \log\pi_\theta(y_l|x) \right] $$
                                </div>
                                
                                <p><strong>Breaking down the components:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>\([\nabla_\theta \log\pi_\theta(y_w|x) - \nabla_\theta \log\pi_\theta(y_l|x)]\):</strong> The update direction</li>
                                    <li><strong>\(\sigma(\hat{r}_\theta(y_l) - \hat{r}_\theta(y_w))\):</strong> The adaptive weighting factor</li>
                                    <li><strong>\(-\beta\):</strong> Scale factor (negative because we minimize loss)</li>
                                </ul>
                                
                                <p><strong>The Adaptive Weighting Magic:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>When model is wrong:</strong> \(\hat{r}_\theta(y_l) > \hat{r}_\theta(y_w)\) → weight ≈ 1.0 → large updates</li>
                                    <li><strong>When model is right:</strong> \(\hat{r}_\theta(y_w) > \hat{r}_\theta(y_l)\) → weight ≈ 0.0 → small updates</li>
                                    <li><strong>When model is uncertain:</strong> \(\hat{r}_\theta(y_w) \approx \hat{r}_\theta(y_l)\) → weight ≈ 0.5 → medium updates</li>
                                </ul>
                                
                                <p><strong>Why this is brilliant:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>🎯 Automatic focus:</strong> Spends more effort on examples the model gets wrong</li>
                                    <li><strong>🛡️ Stability:</strong> Reduces updates when model is already correct</li>
                                    <li><strong>⚖️ Self-regulating:</strong> No need to manually tune learning rates for different examples</li>
                                    <li><strong>🎪 Efficient:</strong> Doesn't waste time on easy examples</li>
                                </ul>
                                
                                <p><strong>Comparison to Standard Classification:</strong> Unlike regular classification which applies uniform updates, DPO automatically adapts the learning intensity based on the model's current performance on each example.</p>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Adaptive Learning in Action</h6>
                                <strong>Scenario:</strong> Training a coding assistant to give better programming advice<br><br>
                                <strong>Coding Question:</strong> "How should I handle errors in my Python code?"<br><br>
                                <strong>Scenario 1: Model is Wrong (Needs Big Updates)</strong><br>
                                • Winner: "Use try-except blocks to catch and handle specific exceptions gracefully"<br>
                                • Loser: "Just ignore errors, they usually don't matter"<br>
                                • Model currently prefers the loser! \(\hat{r}_w = 0.1\), \(\hat{r}_l = 0.3\)<br>
                                • Gradient weight: \(\sigma(0.3 - 0.1) = 0.55\) (55% of maximum)<br>
                                • Result: Strong updates to fix this dangerous misconception<br><br>
                                <strong>Scenario 2: Model is Right (Moderate Updates)</strong><br>
                                • Winner: "Use descriptive error messages and log exceptions for debugging"<br>
                                • Loser: "Print 'error occurred' when something goes wrong"<br>
                                • Model correctly prefers winner: \(\hat{r}_w = 0.4\), \(\hat{r}_l = 0.1\)<br>
                                • Gradient weight: \(\sigma(0.1 - 0.4) = 0.43\) (43% of maximum)<br>
                                • Result: Moderate reinforcement of good practices<br><br>
                                <strong>Scenario 3: Model is Very Confident and Right (Small Updates)</strong><br>
                                • Winner: "Use specific exception types like FileNotFoundError instead of bare except"<br>
                                • Loser: "Use bare except: pass to ignore all errors"<br>
                                • Model strongly prefers winner: \(\hat{r}_w = 0.8\), \(\hat{r}_l = -0.2\)<br>
                                • Gradient weight: \(\sigma(-0.2 - 0.8) = 0.27\) (27% of maximum)<br>
                                • Result: Small updates, model already knows this well<br><br>
                                <strong>Adaptive Learning Benefit:</strong> DPO automatically focuses effort where the model needs it most!
                            </div>
                        </div>
                    </div>

                    <div class="workflow-card" data-step="6">
                        <h5>DPO Implementation: From Theory to Code</h5>
                        <p class="card-explainer">Understanding how DPO is implemented in practice helps bridge the gap between mathematical theory and real-world training.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>DPO Training Loop: The Practical Implementation</h6>
                                <p><strong>Data Preparation:</strong> Each training example contains \((x, y_w, y_l)\):</p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Prompt tokens:</strong> \(x = [\text{token}_1, \text{token}_2, ..., \text{token}_n]\)</li>
                                    <li><strong>Chosen response:</strong> \(y_w = [\text{token}_{n+1}, ..., \text{token}_{n+m}]\)</li>
                                    <li><strong>Rejected response:</strong> \(y_l = [\text{token}_{n+1}, ..., \text{token}_{n+k}]\)</li>
                                </ul>
                                
                                <p><strong>Forward Pass Efficiency Trick:</strong></p>
                                <p>Instead of two separate forward passes, concatenate chosen and rejected responses:</p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ \text{batch} = \begin{bmatrix} [x, y_w] \\ [x, y_l] \end{bmatrix} \rightarrow \text{single forward pass} $$
                                </div>
                                
                                <p><strong>Log Probability Computation:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Policy model:</strong> \(\log \pi_\theta(y|x) = \sum_{t} \log \pi_\theta(y_t | x, y_{<t})\)</li>
                                    <li><strong>Reference model:</strong> \(\log \pi_{\text{ref}}(y|x)\) (with gradients disabled)</li>
                                    <li><strong>Efficiency:</strong> Can use same model with/without adapters for reference</li>
                                </ul>
                                
                                <p><strong>Memory Optimization Strategies:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>PEFT (LoRA):</strong> Only train small adapter layers, not full model</li>
                                    <li><strong>Gradient checkpointing:</strong> Trade compute for memory</li>
                                    <li><strong>Reference-free mode:</strong> Skip reference model entirely (less stable)</li>
                                    <li><strong>Mixed precision:</strong> Use fp16/bf16 for memory savings</li>
                                </ul>
                                
                                <p><strong>Training Stability Tips:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Label smoothing:</strong> Mix target with uniform distribution</li>
                                    <li><strong>Beta scheduling:</strong> Start with higher \(\beta\), decay over time</li>
                                    <li><strong>Learning rate warmup:</strong> Gradual increase to prevent instability</li>
                                </ul>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Email Assistant Training</h6>
                                <strong>Scenario:</strong> Training an AI email assistant using DPO<br><br>
                                <strong>Training Configuration:</strong><br>
                                • Base model: Llama-2 7B (business communication specialist)<br>
                                • Method: LoRA adapters (efficient fine-tuning)<br>
                                • Beta: 0.1 (balance between exploration and exploitation)<br>
                                • Learning rate: 5e-4 (faster learning than traditional RLHF)<br>
                                • Batch size: 64 email preference pairs per update<br>
                                • Sequence length: 2048 tokens (full email conversations)<br><br>
                                <strong>Sample Training Pair:</strong><br>
                                <strong>Prompt:</strong> "Write a professional email declining a meeting request"<br><br>
                                <strong>Winner:</strong> "Thank you for the meeting invitation. Unfortunately, I have a scheduling conflict and won't be able to attend. Could we explore alternative dates next week? I'm available Tuesday-Thursday afternoons."<br>
                                • Professional, specific, offers alternatives<br><br>
                                <strong>Loser:</strong> "Can't make it."<br>
                                • Too brief, unprofessional, unhelpful<br><br>
                                <strong>Training Efficiency Benefits:</strong><br>
                                • Training time: 8-12 hours (vs 3-5 days for RLHF)<br>
                                • Simpler pipeline: No separate reward model training<br>
                                • Stable learning: No PPO hyperparameter sensitivity<br>
                                • Direct optimization: Learns exactly what humans prefer<br><br>
                                <strong>Result:</strong> Email assistant learns professional communication patterns 10x faster than traditional RLHF
                            </div>
                        </div>
                    </div>
                </div>
                <div class="card-navigation">
                    <button id="dpo-prev" disabled>Previous</button>
                    <div class="step-indicators">
                        <span class="dot active" data-step="1"></span>
                        <span class="dot" data-step="2"></span>
                        <span class="dot" data-step="3"></span>
                        <span class="dot" data-step="4"></span>
                        <span class="dot" data-step="5"></span>
                        <span class="dot" data-step="6"></span>
                        <span class="dot" data-step="7"></span>
                        <span class="dot" data-step="8"></span>
                        <span class="dot" data-step="9"></span>
                        <span class="dot" data-step="10"></span>
                    </div>
                    <button id="dpo-next">Next</button>
                </div>
            </div>
        </div>

        <div class="container">
            <h2>3. An Efficient RL Approach: Group Relative Policy Optimization (GRPO)</h2>
            <p>As pioneered by DeepSeek-AI, <strong>Group Relative Policy Optimization (GRPO)</strong> is an efficient RL algorithm that eliminates the need for a separate, expensive critic model (common in PPO). Instead, it normalizes rewards by comparing a "group" of different responses generated for the same prompt. Below is an interactive walkthrough of GRPO's key innovations and mathematical foundations.</p>
            <p>This makes it highly effective for improving reasoning, where a simple, rule-based reward (e.g., "is the final answer correct?") can be used. The "group" allows the model to assign relative credit to different attempts, even if they all lead to the same binary outcome.</p>
            
            <h3>GRPO Workflow</h3>
            <div class="grpo-workflow-container">
                <div class="workflow-mini">
                    <div class="workflow-step">Prompt (x)</div>
                    <div class="workflow-arrow-down">&darr;</div>
                    <div class="workflow-step">Policy Model (π)</div>
                    <div class="workflow-arrow-down">&darr;</div>
                    <div class="workflow-step">Generate a <b>Group</b> of N Completions</div>
                </div>
                <div class="completions-container">
                    <div class="workflow-step">y<sub>1</sub></div>
                    <div class="workflow-step">y<sub>2</sub></div>
                    <div class="workflow-step">...</div>
                    <div class="workflow-step">y<sub>N</sub></div>
                </div>
                 <div class="workflow-arrow-down">&darr;</div>
                <div class="workflow-step">Score each completion with a Reward Function (r)</div>
                 <div class="workflow-arrow-down">&darr;</div>
                <div class="workflow-step highlight2">Normalize rewards across the group to compute <b>Advantage (A<sub>i</sub>)</b></div>
                 <div class="workflow-arrow-down">&darr;</div>
                <div class="workflow-step final">Update Policy (π) using Advantage</div>
            </div>
            
            <div id="grpo-interactive-workflow">
                <div class="card-container">
                    <div class="workflow-card active" data-step="1">
                        <h5>GRPO Core Innovation: Critic-Free Learning</h5>
                        <p class="card-explainer">GRPO eliminates the need for a separate value function (critic) by using group-based reward normalization to create learning signals.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>The Problem with Traditional RL: Expensive Critics</h6>
                                <p><strong>PPO's Challenge:</strong> Imagine you're training a student, but you need a separate teacher to constantly evaluate how good each answer is. That's expensive and complex!</p>
                                
                                <p><strong>PPO Requires:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Policy Network \(\pi_\theta\):</strong> The student (generates responses)</li>
                                    <li><strong>Value Network \(V_\phi\):</strong> The teacher (predicts how good responses will be)</li>
                                    <li><strong>Both need training:</strong> Double the computational cost</li>
                                    <li><strong>Complex interactions:</strong> Teacher and student must learn together</li>
                                </ul>
                                
                                <p><strong>GRPO's Brilliant Solution:</strong></p>
                                <p>"What if we don't need a separate teacher? What if students can learn by comparing their work with classmates?"</p>
                                
                                <p><strong>Group-Based Learning:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Generate multiple responses:</strong> Like having 4 students solve the same problem</li>
                                    <li><strong>Compare within the group:</strong> See who did better or worse</li>
                                    <li><strong>Learn from comparisons:</strong> "I should do more like Sarah, less like Bob"</li>
                                    <li><strong>No external judge needed:</strong> The group IS the teacher</li>
                                </ul>
                                
                                <p><strong>Key Insight:</strong> You don't need to know absolute quality scores. You just need to know "this response is better than that response" within the same context.</p>
                                
                                <p><strong>Benefits:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li>🚀 <strong>Faster:</strong> No critic training needed</li>
                                    <li>💰 <strong>Cheaper:</strong> Half the computational cost</li>
                                    <li>🎯 <strong>Simpler:</strong> One model, one objective</li>
                                    <li>🎪 <strong>Effective:</strong> Works especially well for reasoning tasks</li>
                                </ul>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Math Competition Scenario</h6>
                                <strong>Scenario:</strong> Training an AI math tutor using GRPO vs PPO<br><br>
                                <strong>Problem:</strong> "Solve: 3x + 7 = 22"<br><br>
                                <strong>PPO Approach (Complex):</strong><br>
                                1. Policy generates response: "3x = 15, so x = 5"<br>
                                2. Critic evaluates: "This looks like it will get reward 0.8"<br>
                                3. Actual reward: 1.0 (correct)<br>
                                4. Update both policy AND critic<br>
                                5. Critic learns to predict better, policy learns from critic<br><br>
                                <strong>GRPO Approach (Simple):</strong><br>
                                1. Generate 4 responses simultaneously:<br>
                                   • "x = 5" (correct)<br>
                                   • "x = 5" (correct, different method)<br>
                                   • "x = 7" (wrong)<br>
                                   • "x = 3" (wrong)<br>
                                2. Compare within group: 2 correct, 2 wrong<br>
                                3. Learn directly: "Do more like the correct ones"<br>
                                4. No critic needed!<br><br>
                                <strong>Result:</strong> GRPO achieves similar learning with half the complexity and computational cost
                            </div>
                        </div>
                    </div>

                    <div class="workflow-card" data-step="2">
                        <h5>Group Generation: Creating Diverse Responses</h5>
                        <p class="card-explainer">GRPO's first step is generating a diverse group of responses to the same prompt, creating a rich comparison set for learning.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>Step 1: Sampling Strategy for Diverse Groups</h6>
                                <p><strong>The Goal:</strong> Generate \(G\) different responses that explore various approaches to the same problem.</p>
                                
                                <p><strong>Sampling Parameters:</strong></p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ \{o_1, o_2, ..., o_G\} \sim \pi_\theta(\cdot | x) $$
                                </div>
                                
                                <p><strong>Where:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>\(x\):</strong> The input prompt (same for all responses)</li>
                                    <li><strong>\(o_i\):</strong> The i-th generated response</li>
                                    <li><strong>\(G\):</strong> Group size (typically 4-8 responses)</li>
                                    <li><strong>\(\pi_\theta\):</strong> Current policy (the model being trained)</li>
                                </ul>
                                
                                <p><strong>Diversity Techniques:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Temperature sampling:</strong> \(T > 1\) for more creativity</li>
                                    <li><strong>Top-k sampling:</strong> Consider multiple high-probability tokens</li>
                                    <li><strong>Multiple random seeds:</strong> Ensure different reasoning paths</li>
                                </ul>
                                
                                <p><strong>Why Diversity Matters:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Exploration:</strong> Discover new solution strategies</li>
                                    <li><strong>Comparison:</strong> Need both good and bad examples to learn</li>
                                    <li><strong>Robustness:</strong> Learn to handle different approaches</li>
                                    <li><strong>Signal strength:</strong> More variance = stronger learning signal</li>
                                </ul>
                                
                                <p><strong>Group Size Trade-offs:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Small groups (G=2-3):</strong> Fast, but limited comparison</li>
                                    <li><strong>Medium groups (G=4-6):</strong> Good balance of speed and signal</li>
                                    <li><strong>Large groups (G=8+):</strong> Rich comparisons, but expensive</li>
                                </ul>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Physics Problem Solving</h6>
                                <strong>Scenario:</strong> Training an AI physics tutor with diverse reasoning approaches<br><br>
                                <strong>Physics Problem:</strong> "A ball is thrown upward with initial velocity 20 m/s. How high does it go?"<br><br>
                                <strong>GRPO Group Generation (G=4):</strong><br><br>
                                <strong>Response 1 (Kinematic Equations):</strong><br>
                                "Using v² = u² + 2as, where final velocity v=0 at max height, initial velocity u=20 m/s, acceleration a=-9.8 m/s². So 0 = 400 + 2(-9.8)s, giving s = 400/19.6 = 20.4 meters."<br><br>
                                <strong>Response 2 (Energy Conservation):</strong><br>
                                "Using energy conservation: initial kinetic energy = final potential energy. ½mv² = mgh, so ½(20)² = 9.8h, giving h = 400/19.6 = 20.4 meters."<br><br>
                                <strong>Response 3 (Time-based Approach):</strong><br>
                                "Time to reach max height: v = u + at, so 0 = 20 - 9.8t, giving t = 2.04 seconds. Height: s = ut + ½at² = 20(2.04) + ½(-9.8)(2.04)² = 20.4 meters."<br><br>
                                <strong>Response 4 (Common Error):</strong><br>
                                "Using s = ut + ½at², with t=2 seconds: s = 20(2) + ½(-9.8)(4) = 40 - 19.6 = 20.4 meters."<br>
                                (Error: assumed t=2 without calculating)<br><br>
                                <strong>Diversity Achieved:</strong> Three valid methods plus one common mistake, providing rich learning opportunities
                            </div>
                        </div>
                    </div>

                    <div class="workflow-card" data-step="3">
                        <h5>Reward Evaluation: Simple Rule-Based Scoring</h5>
                        <p class="card-explainer">GRPO works best with simple, objective reward functions that can be easily computed for each response in the group.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>Step 2: Computing Individual Rewards</h6>
                                <p><strong>Reward Function Design:</strong> GRPO excels with simple, rule-based rewards that are easy to compute and objective.</p>
                                
                                <p><strong>Common Reward Types:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Accuracy:</strong> \(r_i = 1\) if correct, \(0\) if wrong</li>
                                    <li><strong>Format compliance:</strong> \(r_i = 1\) if follows required format</li>
                                    <li><strong>Code execution:</strong> \(r_i = 1\) if code runs without errors</li>
                                    <li><strong>Verification:</strong> \(r_i = 1\) if answer can be verified</li>
                                </ul>
                                
                                <p><strong>Mathematical Formulation:</strong></p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ r_i = R(x, o_i) $$
                                </div>
                                
                                <p><strong>Where:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>\(R(\cdot)\):</strong> Reward function (often rule-based)</li>
                                    <li><strong>\(x\):</strong> Input prompt</li>
                                    <li><strong>\(o_i\):</strong> i-th generated response</li>
                                    <li><strong>\(r_i\):</strong> Scalar reward for response i</li>
                                </ul>
                                
                                <p><strong>Example Reward Functions:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Math problems:</strong> \(R = 1\) if final answer matches ground truth</li>
                                    <li><strong>Code generation:</strong> \(R = 1\) if code passes all test cases</li>
                                    <li><strong>Reasoning tasks:</strong> \(R = 1\) if conclusion is logically valid</li>
                                    <li><strong>Format tasks:</strong> \(R = 1\) if uses required tags (&lt;think&gt;, &lt;answer&gt;)</li>
                                </ul>
                                
                                <p><strong>Why Simple Rewards Work:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Objective:</strong> No human judgment needed</li>
                                    <li><strong>Fast:</strong> Can be computed automatically</li>
                                    <li><strong>Clear signal:</strong> Binary rewards give strong learning signals</li>
                                    <li><strong>Scalable:</strong> Works for thousands of examples</li>
                                </ul>
                                
                                <p><strong>Composite Rewards:</strong></p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ r_i = w_1 \cdot R_{\text{accuracy}}(o_i) + w_2 \cdot R_{\text{format}}(o_i) + w_3 \cdot R_{\text{style}}(o_i) $$
                                </div>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Coding Assistant Evaluation</h6>
                                <strong>Scenario:</strong> Training an AI coding assistant with multi-component rewards<br><br>
                                <strong>Programming Task:</strong> "Write a Python function to find the factorial of a number"<br><br>
                                <strong>Group Responses with Reward Breakdown:</strong><br><br>
                                <strong>Response 1 (Excellent):</strong><br>
                                <code>def factorial(n):<br>
                                &nbsp;&nbsp;if n == 0: return 1<br>
                                &nbsp;&nbsp;return n * factorial(n-1)</code><br>
                                • Accuracy: 1.0 (correct algorithm)<br>
                                • Format: 1.0 (proper Python syntax)<br>
                                • Style: 1.0 (handles edge case)<br>
                                • <strong>Total Reward: 3.0</strong><br><br>
                                <strong>Response 2 (Good):</strong><br>
                                <code>def factorial(n):<br>
                                &nbsp;&nbsp;result = 1<br>
                                &nbsp;&nbsp;for i in range(1, n+1):<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;result *= i<br>
                                &nbsp;&nbsp;return result</code><br>
                                • Accuracy: 1.0 (correct iterative approach)<br>
                                • Format: 1.0 (proper syntax)<br>
                                • Style: 0.5 (doesn't handle n=0 case)<br>
                                • <strong>Total Reward: 2.5</strong><br><br>
                                <strong>Response 3 (Syntax Error):</strong><br>
                                <code>def factorial(n)<br>
                                &nbsp;&nbsp;return n * factorial(n-1)</code><br>
                                • Accuracy: 0.0 (missing colon, infinite recursion)<br>
                                • Format: 0.0 (syntax error)<br>
                                • Style: 0.0 (no base case)<br>
                                • <strong>Total Reward: 0.0</strong><br><br>
                                <strong>Response 4 (Wrong Algorithm):</strong><br>
                                <code>def factorial(n):<br>
                                &nbsp;&nbsp;return n * n</code><br>
                                • Accuracy: 0.0 (wrong algorithm)<br>
                                • Format: 1.0 (valid syntax)<br>
                                • Style: 0.0 (not factorial)<br>
                                • <strong>Total Reward: 1.0</strong>
                            </div>
                        </div>
                    </div>

                    <div class="workflow-card" data-step="4">
                        <h5>Advantage Calculation: Group-Based Normalization</h5>
                        <p class="card-explainer">The core of GRPO: converting raw rewards into normalized advantages by comparing each response against the group average.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>Step 3: The GRPO Advantage Formula</h6>
                                <p><strong>The Heart of GRPO:</strong> Transform raw rewards into learning signals by normalizing within each group.</p>
                                
                                <p><strong>Group Statistics:</strong></p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ \mu_G = \frac{1}{G} \sum_{i=1}^G r_i \quad \text{(Group Mean)} $$
                                </div>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ \sigma_G = \sqrt{\frac{1}{G} \sum_{i=1}^G (r_i - \mu_G)^2} \quad \text{(Group Std Dev)} $$
                                </div>
                                
                                <p><strong>GRPO Advantage:</strong></p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ A_i = \frac{r_i - \mu_G}{\sigma_G + \epsilon} $$
                                </div>
                                
                                <p><strong>What This Does:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>\(A_i > 0\):</strong> Response i is better than group average</li>
                                    <li><strong>\(A_i = 0\):</strong> Response i is exactly average for this group</li>
                                    <li><strong>\(A_i < 0\):</strong> Response i is worse than group average</li>
                                    <li><strong>\(\epsilon\):</strong> Small constant (1e-8) to prevent division by zero</li>
                                </ul>
                                
                                <p><strong>Why Normalization is Brilliant:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Relative learning:</strong> "Be more like the good responses in this group"</li>
                                    <li><strong>Automatic scaling:</strong> Works whether rewards are 0-1 or 0-100</li>
                                    <li><strong>Variance handling:</strong> High variance groups get smaller updates</li>
                                    <li><strong>Zero-sum property:</strong> \(\sum A_i = 0\) (advantages balance out)</li>
                                </ul>
                                
                                <p><strong>Edge Cases:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>All rewards equal:</strong> \(\sigma_G = 0\), so \(A_i = 0\) (no learning)</li>
                                    <li><strong>High variance:</strong> Large \(\sigma_G\) reduces advantage magnitude</li>
                                    <li><strong>Binary rewards:</strong> Creates clear +/- advantages</li>
                                </ul>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Creative Writing Assessment</h6>
                                <strong>Scenario:</strong> Training an AI creative writer with GRPO advantage calculation<br><br>
                                <strong>Writing Prompt:</strong> "Write a compelling opening sentence for a mystery novel"<br><br>
                                <strong>Group Responses (G=4) with Quality Scores:</strong><br><br>
                                <strong>Response 1:</strong> "Detective Sarah Chen knew the moment she saw the pristine kitchen that someone had died here."<br>
                                • Quality score: \(r_1 = 4.2\) (engaging, mysterious)<br><br>
                                <strong>Response 2:</strong> "The murder weapon lay in plain sight, which was exactly what made it invisible."<br>
                                • Quality score: \(r_2 = 4.8\) (clever paradox, intriguing)<br><br>
                                <strong>Response 3:</strong> "There was a dead body in the room."<br>
                                • Quality score: \(r_3 = 2.1\) (bland, uninspiring)<br><br>
                                <strong>Response 4:</strong> "The clock struck midnight as the mystery began to unfold."<br>
                                • Quality score: \(r_4 = 3.5\) (clichéd but functional)<br><br>
                                <strong>GRPO Advantage Calculation:</strong><br>
                                • Group mean: \(\mu_G = (4.2 + 4.8 + 2.1 + 3.5) ÷ 4 = 3.65\)<br>
                                • Group std dev: \(\sigma_G = 1.02\)<br>
                                • Advantages:<br>
                                &nbsp;&nbsp;- \(A_1 = (4.2 - 3.65) ÷ 1.02 = +0.54\) (above average)<br>
                                &nbsp;&nbsp;- \(A_2 = (4.8 - 3.65) ÷ 1.02 = +1.13\) (much above average)<br>
                                &nbsp;&nbsp;- \(A_3 = (2.1 - 3.65) ÷ 1.02 = -1.52\) (well below average)<br>
                                &nbsp;&nbsp;- \(A_4 = (3.5 - 3.65) ÷ 1.02 = -0.15\) (slightly below average)<br><br>
                                <strong>Learning Signal:</strong> Model learns to strongly favor Response 2's style, moderately favor Response 1, avoid Response 3's blandness, and slightly discourage Response 4's clichés
                            </div>
                        </div>
                    </div>

                    <div class="workflow-card" data-step="5">
                        <h5>GRPO Objective Function: Policy Updates</h5>
                        <p class="card-explainer">GRPO uses the computed advantages to update the policy, encouraging responses with positive advantages and discouraging those with negative advantages.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>Step 4: The Complete GRPO Objective</h6>
                                <p><strong>GRPO Loss Function:</strong> Combines PPO-style clipping with group-based advantages</p>
                                
                                <div class="equation math-jax" style="font-size: 0.75em; padding: 6px; margin: 6px 0;">
                                    $$ \mathcal{J}_{\text{GRPO}}(\theta) = \frac{1}{G}\sum_{i=1}^G \left[ \min \left( r_i(\theta) A_i, \text{clip}(r_i(\theta), 1-\epsilon, 1+\epsilon) A_i \right) - \beta \mathbb{D}_{KL} \right] $$
                                </div>
                                
                                <p><strong>Component Breakdown:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>\(r_i(\theta) = \frac{\pi_\theta(o_i|x)}{\pi_{\theta_{old}}(o_i|x)}\):</strong> Importance sampling ratio</li>
                                    <li><strong>\(A_i\):</strong> Group-normalized advantage (our key innovation)</li>
                                    <li><strong>\(\text{clip}(\cdot, 1-\epsilon, 1+\epsilon)\):</strong> PPO clipping for stability</li>
                                    <li><strong>\(\beta \mathbb{D}_{KL}\):</strong> KL penalty to stay close to reference policy</li>
                                </ul>
                                
                                <p><strong>How Learning Works:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Positive advantage (\(A_i > 0\)):</strong> Increase probability of generating \(o_i\)</li>
                                    <li><strong>Negative advantage (\(A_i < 0\)):</strong> Decrease probability of generating \(o_i\)</li>
                                    <li><strong>Clipping:</strong> Prevents too-large policy updates</li>
                                    <li><strong>KL penalty:</strong> Maintains coherent language generation</li>
                                </ul>
                                
                                <p><strong>Key Differences from PPO:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>No critic network:</strong> Advantages come from group comparison</li>
                                    <li><strong>Batch-based learning:</strong> All group members updated together</li>
                                    <li><strong>Relative rewards:</strong> Focus on "better than peers" not "absolutely good"</li>
                                    <li><strong>Simpler pipeline:</strong> One model, one objective function</li>
                                </ul>
                                
                                <p><strong>Gradient Intuition:</strong></p>
                                <p>For each response \(o_i\):</p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ \nabla_\theta \mathcal{J} \propto A_i \cdot \nabla_\theta \log \pi_\theta(o_i | x) $$
                                </div>
                                <p>This pushes the model toward generating responses similar to high-advantage examples and away from low-advantage ones.</p>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Code Debugging Assistant</h6>
                                <strong>Scenario:</strong> Training an AI to debug Python code using GRPO<br><br>
                                <strong>Bug Report:</strong> "My code gives 'IndexError: list index out of range'"<br>
                                <strong>Buggy Code:</strong> <code>nums = [1,2,3]; print(nums[5])</code><br><br>
                                <strong>Group Responses (G=4):</strong><br><br>
                                <strong>Response 1 (Excellent Fix + Explanation):</strong><br>
                                "The error occurs because you're trying to access index 5 in a list that only has indices 0-2. Fix: Check if index exists: <code>if 5 < len(nums): print(nums[5])</code>"<br>
                                • Reward: 3.0, Advantage: +1.2<br><br>
                                <strong>Response 2 (Good Fix, Less Explanation):</strong><br>
                                "Index 5 doesn't exist. Use <code>print(nums[2])</code> for the last element."<br>
                                • Reward: 2.5, Advantage: +0.6<br><br>
                                <strong>Response 3 (Wrong Solution):</strong><br>
                                "Try using <code>nums.append(0, 0)</code> to add more elements."<br>
                                • Reward: 0.5, Advantage: -1.4<br><br>
                                <strong>Response 4 (Partial Understanding):</strong><br>
                                "There's an index error. Check your list length."<br>
                                • Reward: 1.5, Advantage: -0.4<br><br>
                                <strong>GRPO Update Calculation:</strong><br>
                                • Response 1: Large positive update (encourage detailed explanations)<br>
                                • Response 2: Moderate positive update (encourage correct fixes)<br>
                                • Response 3: Large negative update (discourage wrong solutions)<br>
                                • Response 4: Small negative update (discourage vague answers)<br><br>
                                <strong>Learning Outcome:</strong> Model learns to provide detailed, accurate debugging advice with clear explanations and correct code fixes
                            </div>
                        </div>
                    </div>

                    <div class="workflow-card" data-step="6">
                        <h5>GRPO Training Loop: Complete Implementation</h5>
                        <p class="card-explainer">Understanding the full GRPO training loop from data collection through policy updates, including batch processing and efficiency optimizations.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>Step 5: The Complete GRPO Training Algorithm</h6>
                                <p><strong>Training Loop Overview:</strong> GRPO follows a streamlined process that's simpler than PPO but highly effective for reasoning tasks.</p>
                                
                                <p><strong>Algorithm Structure:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>1. Batch Collection:</strong> Sample B prompts from training set</li>
                                    <li><strong>2. Group Generation:</strong> For each prompt, generate G responses</li>
                                    <li><strong>3. Reward Evaluation:</strong> Score all B×G responses</li>
                                    <li><strong>4. Advantage Computation:</strong> Normalize rewards within each group</li>
                                    <li><strong>5. Policy Update:</strong> Single gradient step using all advantages</li>
                                    <li><strong>6. Repeat:</strong> Continue for desired number of iterations</li>
                                </ul>
                                
                                <p><strong>Batch Processing Efficiency:</strong></p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ \text{Total Responses} = B \times G $$
                                </div>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ \text{Gradient} = \frac{1}{B \times G} \sum_{b=1}^B \sum_{g=1}^G \nabla_\theta \mathcal{L}(o_{b,g}) $$
                                </div>
                                
                                <p><strong>Key Implementation Details:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Parallel generation:</strong> All G responses generated simultaneously</li>
                                    <li><strong>Vectorized rewards:</strong> Batch compute all rewards at once</li>
                                    <li><strong>Memory efficiency:</strong> Process groups independently</li>
                                    <li><strong>Gradient accumulation:</strong> Sum gradients across all groups</li>
                                </ul>
                                
                                <p><strong>Hyperparameter Sensitivity:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Group size G:</strong> 4-8 optimal for most tasks</li>
                                    <li><strong>Batch size B:</strong> 32-128 prompts per iteration</li>
                                    <li><strong>Learning rate:</strong> 1e-6 to 5e-6 (similar to PPO)</li>
                                    <li><strong>KL coefficient β:</strong> 0.01-0.05 for stability</li>
                                </ul>
                                
                                <p><strong>Convergence Properties:</strong> GRPO typically converges 2-3x faster than PPO on reasoning tasks due to the direct group comparison providing stronger learning signals.</p>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Algorithm Implementation</h6>
                                <strong>Scenario:</strong> Training a math solver with GRPO implementation details<br><br>
                                <strong>Training Configuration:</strong><br>
                                • Batch size B = 64 prompts<br>
                                • Group size G = 6 responses per prompt<br>
                                • Total responses per iteration = 384<br>
                                • Learning rate = 2e-6<br>
                                • KL penalty β = 0.02<br><br>
                                <strong>Single Training Iteration:</strong><br><br>
                                <strong>Step 1: Batch Collection (0.1s)</strong><br>
                                • Sample 64 math problems from training set<br>
                                • Problems range from algebra to calculus<br><br>
                                <strong>Step 2: Parallel Generation (2.3s)</strong><br>
                                • Generate 6 solutions per problem simultaneously<br>
                                • Use temperature T=1.2 for diversity<br>
                                • Total: 384 mathematical solutions<br><br>
                                <strong>Step 3: Reward Computation (0.8s)</strong><br>
                                • Check final answers against ground truth<br>
                                • Verify mathematical reasoning steps<br>
                                • Assign binary rewards (1.0 or 0.0)<br><br>
                                <strong>Step 4: Advantage Calculation (0.1s)</strong><br>
                                • Normalize rewards within each group of 6<br>
                                • Example group: rewards [1,1,0,1,0,0] → advantages [+0.8,+0.8,-1.2,+0.8,-1.2,-1.2]<br><br>
                                <strong>Step 5: Policy Update (1.2s)</strong><br>
                                • Compute gradients for all 384 responses<br>
                                • Apply clipping and KL penalty<br>
                                • Single optimizer step<br><br>
                                <strong>Total Time: 4.5s per iteration</strong> (vs 8-12s for PPO)
                            </div>
                        </div>
                    </div>

                    <div class="workflow-card" data-step="7">
                        <h5>Advanced GRPO Techniques: Curriculum and Adaptive Sampling</h5>
                        <p class="card-explainer">Sophisticated techniques to enhance GRPO performance, including curriculum learning, adaptive group sizes, and dynamic reward weighting.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>Advanced Optimization Strategies</h6>
                                <p><strong>1. Curriculum Learning with GRPO:</strong></p>
                                <p>Start with easier problems and gradually increase difficulty as the model improves.</p>
                                
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ \text{Difficulty}(t) = \min(D_{\max}, D_0 + \alpha \cdot t) $$
                                </div>
                                
                                <p><strong>2. Adaptive Group Size:</strong></p>
                                <p>Adjust group size based on model performance - larger groups when model is uncertain, smaller when confident.</p>
                                
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ G_{\text{adaptive}} = G_{\min} + (G_{\max} - G_{\min}) \cdot \text{Uncertainty}(\pi_\theta) $$
                                </div>
                                
                                <p><strong>3. Temperature Scheduling:</strong></p>
                                <p>Start with high temperature for exploration, gradually reduce for exploitation.</p>
                                
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ T(t) = T_{\max} \cdot \exp(-\lambda t) + T_{\min} $$
                                </div>
                                
                                <p><strong>4. Multi-Objective Rewards:</strong></p>
                                <p>Combine multiple reward signals for richer learning.</p>
                                
                                <div class="equation math-jax" style="font-size: 0.75em; padding: 6px; margin: 6px 0;">
                                    $$ r_{\text{total}} = w_1 r_{\text{accuracy}} + w_2 r_{\text{elegance}} + w_3 r_{\text{efficiency}} + w_4 r_{\text{explanation}} $$
                                </div>
                                
                                <p><strong>5. Dynamic KL Weighting:</strong></p>
                                <p>Adjust KL penalty based on training progress.</p>
                                
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ \beta(t) = \beta_0 \cdot \left(1 + \gamma \cdot \text{Performance}(t)\right) $$
                                </div>
                                
                                <p><strong>6. Rejection Sampling Integration:</strong></p>
                                <p>Use high-quality GRPO outputs to create SFT data for the next training phase.</p>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Advanced Training Pipeline</h6>
                                <strong>Scenario:</strong> Training an advanced mathematical reasoning system<br><br>
                                <strong>Phase 1: Curriculum Introduction (Weeks 1-2)</strong><br>
                                • Start with basic arithmetic: "What is 15 + 27?"<br>
                                • Group size: G = 8 (high exploration)<br>
                                • Temperature: T = 1.5 (very diverse responses)<br>
                                • Success rate: 60% → 85%<br><br>
                                <strong>Phase 2: Intermediate Problems (Weeks 3-4)</strong><br>
                                • Move to algebra: "Solve 2x² - 5x + 2 = 0"<br>
                                • Group size: G = 6 (moderate exploration)<br>
                                • Temperature: T = 1.2 (balanced diversity)<br>
                                • Success rate: 40% → 75%<br><br>
                                <strong>Phase 3: Advanced Reasoning (Weeks 5-8)</strong><br>
                                • Complex proofs: "Prove the fundamental theorem of calculus"<br>
                                • Group size: G = 4 (focused generation)<br>
                                • Temperature: T = 0.9 (more deterministic)<br>
                                • Success rate: 20% → 70%<br><br>
                                <strong>Multi-Objective Reward Example:</strong><br>
                                Problem: "Prove that √2 is irrational"<br>
                                • Accuracy: 1.0 (proof is correct)<br>
                                • Elegance: 0.8 (uses contradiction method elegantly)<br>
                                • Efficiency: 0.9 (concise, no unnecessary steps)<br>
                                • Explanation: 0.7 (clear but could be more beginner-friendly)<br>
                                • Total reward: 0.4×1.0 + 0.3×0.8 + 0.2×0.9 + 0.1×0.7 = 0.89<br><br>
                                <strong>Adaptive Learning:</strong> As model improves, automatically increase problem difficulty and reduce group size for efficiency
                            </div>
                        </div>
                    </div>

                    <div class="workflow-card" data-step="8">
                        <h5>GRPO vs PPO: Comprehensive Comparison</h5>
                        <p class="card-explainer">Detailed analysis of when and why GRPO outperforms traditional PPO, with empirical results and theoretical insights.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>Comprehensive Performance Analysis</h6>
                                <p><strong>Computational Efficiency Comparison:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>PPO Memory:</strong> Policy (7B) + Critic (7B) = 14B parameters</li>
                                    <li><strong>GRPO Memory:</strong> Policy (7B) only = 7B parameters</li>
                                    <li><strong>Training Speed:</strong> GRPO 2-3x faster on reasoning tasks</li>
                                    <li><strong>Convergence:</strong> GRPO requires 50-70% fewer iterations</li>
                                </ul>
                                
                                <p><strong>Task-Specific Performance:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Mathematical Reasoning:</strong> GRPO +15% accuracy vs PPO</li>
                                    <li><strong>Code Generation:</strong> GRPO +12% pass rate vs PPO</li>
                                    <li><strong>Logic Puzzles:</strong> GRPO +18% success rate vs PPO</li>
                                    <li><strong>Creative Writing:</strong> PPO +8% human preference vs GRPO</li>
                                </ul>
                                
                                <p><strong>Theoretical Advantages of GRPO:</strong></p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ \text{GRPO Variance} = \frac{\sigma^2_{\text{rewards}}}{G} \quad \text{vs} \quad \text{PPO Variance} = \sigma^2_{\text{critic}} + \sigma^2_{\text{rewards}} $$
                                </div>
                                
                                <p><strong>When to Choose GRPO:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>✅ Objective rewards:</strong> Clear right/wrong answers</li>
                                    <li><strong>✅ Resource constraints:</strong> Limited computational budget</li>
                                    <li><strong>✅ Reasoning tasks:</strong> Mathematical, logical, coding problems</li>
                                    <li><strong>✅ Fast iteration:</strong> Need quick experimental cycles</li>
                                </ul>
                                
                                <p><strong>When to Choose PPO:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>✅ Subjective rewards:</strong> Human preference-based tasks</li>
                                    <li><strong>✅ Complex environments:</strong> Multi-step reasoning with delayed rewards</li>
                                    <li><strong>✅ Conversational AI:</strong> Open-ended dialogue systems</li>
                                    <li><strong>✅ Safety-critical:</strong> When stability is more important than efficiency</li>
                                </ul>
                                
                                <p><strong>Hybrid Approaches:</strong> Some systems use GRPO for initial reasoning training, then PPO for fine-tuning conversational abilities.</p>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Empirical Comparison Study</h6>
                                <strong>Scenario:</strong> Head-to-head comparison on mathematical olympiad problems<br><br>
                                <strong>Experimental Setup:</strong><br>
                                • Dataset: 10,000 competition math problems<br>
                                • Base model: Llama-2 7B<br>
                                • Training time: 7 days on 8x A100 GPUs<br>
                                • Evaluation: 1,000 held-out problems<br><br>
                                <strong>PPO Results:</strong><br>
                                • Training time: 168 hours (7 days)<br>
                                • Memory usage: 28GB per GPU (policy + critic)<br>
                                • Final accuracy: 67.3%<br>
                                • Training cost: $2,400 (cloud compute)<br>
                                • Convergence: 15,000 iterations<br><br>
                                <strong>GRPO Results:</strong><br>
                                • Training time: 72 hours (3 days)<br>
                                • Memory usage: 14GB per GPU (policy only)<br>
                                • Final accuracy: 73.8% (+6.5% improvement)<br>
                                • Training cost: $1,200 (50% savings)<br>
                                • Convergence: 8,000 iterations<br><br>
                                <strong>Key Insights:</strong><br>
                                • GRPO's group comparison naturally creates curriculum learning<br>
                                • No critic bias - learns directly from task performance<br>
                                • Better sample efficiency for objective tasks<br>
                                • Simpler hyperparameter tuning<br><br>
                                <strong>Failure Mode Analysis:</strong><br>
                                • GRPO struggles when all group responses are equally bad/good<br>
                                • PPO's critic provides more stable learning for subjective tasks<br>
                                • GRPO requires good reward function design
                            </div>
                        </div>
                    </div>

                    <div class="workflow-card" data-step="9">
                        <h5>DeepSeek-R1 Case Study: GRPO in Practice</h5>
                        <p class="card-explainer">Real-world application of GRPO in DeepSeek-R1, showing how the algorithm scales to state-of-the-art reasoning capabilities.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>DeepSeek-R1's GRPO Implementation</h6>
                                <p><strong>Scale and Configuration:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Base Model:</strong> DeepSeek-V3 (671B parameters)</li>
                                    <li><strong>Training Data:</strong> Millions of reasoning problems</li>
                                    <li><strong>Group Size:</strong> G = 16 for maximum diversity</li>
                                    <li><strong>Batch Size:</strong> B = 512 prompts per iteration</li>
                                    <li><strong>Total Responses:</strong> 8,192 per training step</li>
                                </ul>
                                
                                <p><strong>Multi-Stage GRPO Pipeline:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Stage 1:</strong> Pure GRPO on base model (DeepSeek-R1-Zero)</li>
                                    <li><strong>Stage 2:</strong> GRPO after initial SFT</li>
                                    <li><strong>Stage 3:</strong> GRPO with rejection sampling data</li>
                                    <li><strong>Stage 4:</strong> Final GRPO with multi-objective rewards</li>
                                </ul>
                                
                                <p><strong>Reward Function Evolution:</strong></p>
                                <div class="equation math-jax" style="font-size: 0.75em; padding: 6px; margin: 6px 0;">
                                    $$ r_{\text{DeepSeek}} = w_1 \cdot r_{\text{accuracy}} + w_2 \cdot r_{\text{format}} + w_3 \cdot r_{\text{reasoning}} + w_4 \cdot r_{\text{verification}} $$
                                </div>
                                
                                <p><strong>Key Innovations:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Self-verification rewards:</strong> Bonus for checking own work</li>
                                    <li><strong>Reasoning chain rewards:</strong> Points for clear step-by-step logic</li>
                                    <li><strong>Format consistency:</strong> Proper use of &lt;think&gt; tags</li>
                                    <li><strong>Adaptive difficulty:</strong> Harder problems as model improves</li>
                                </ul>
                                
                                <p><strong>Emergent Behaviors:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Chain-of-thought reasoning:</strong> Emerged naturally from group comparison</li>
                                    <li><strong>Self-correction:</strong> Model learns to catch and fix its own errors</li>
                                    <li><strong>Multiple approaches:</strong> Tries different solution methods</li>
                                    <li><strong>Verification habits:</strong> Always checks final answers</li>
                                </ul>
                                
                                <p><strong>Training Efficiency:</strong> DeepSeek-R1 achieved GPT-4 level reasoning with 3x less compute than traditional RLHF approaches.</p>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: DeepSeek-R1 Training Pipeline</h6>
                                <strong>Scenario:</strong> Training DeepSeek-R1 on complex mathematical reasoning<br><br>
                                <strong>Problem Example:</strong> "Find all real solutions to the equation x⁴ - 5x² + 6 = 0"<br><br>
                                <strong>Stage 1: Base Model GRPO (R1-Zero)</strong><br>
                                Group of 16 responses generated, including:<br>
                                • Response 1: Correct substitution method → x = ±√2, ±√3<br>
                                • Response 8: Attempted factoring but made algebraic error<br>
                                • Response 12: Recognized as quadratic in x² but didn't complete<br>
                                • Response 16: Complete gibberish due to poor initialization<br>
                                Advantage calculation rewards systematic approaches<br><br>
                                <strong>Stage 2: Post-SFT GRPO</strong><br>
                                Now with better baseline, group responses include:<br>
                                • Multiple correct solution methods (substitution, factoring)<br>
                                • Proper mathematical notation and formatting<br>
                                • Verification steps checking solutions in original equation<br>
                                • Clear explanation of solution process<br><br>
                                <strong>Stage 3: Rejection Sampling Integration</strong><br>
                                • Keep only responses with reward ≥ 0.8<br>
                                • Use these for next SFT dataset<br>
                                • Creates virtuous cycle of improvement<br><br>
                                <strong>Final Performance Metrics:</strong><br>
                                • Mathematical accuracy: 94.2% on competition problems<br>
                                • Reasoning clarity: 89.7% human preference score<br>
                                • Self-verification rate: 96.8% (almost always checks work)<br>
                                • Format compliance: 99.1% (proper tag usage)<br><br>
                                <strong>Breakthrough Achievement:</strong> First model to achieve human-level performance on mathematical olympiad problems using pure RL training
                            </div>
                        </div>
                    </div>

                    <div class="workflow-card" data-step="10">
                        <h5>GRPO Implementation: Practical Considerations</h5>
                        <p class="card-explainer">Technical details for implementing GRPO in practice, including code structure, debugging tips, and common pitfalls to avoid.</p>
                        
                        <div class="card-content-grid">
                            <div class="card-math-section">
                                <h6>Implementation Best Practices</h6>
                                <p><strong>Code Structure Overview:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Group Generator:</strong> Parallel sampling with temperature control</li>
                                    <li><strong>Reward Computer:</strong> Vectorized evaluation of all responses</li>
                                    <li><strong>Advantage Calculator:</strong> Group-wise normalization</li>
                                    <li><strong>Policy Updater:</strong> Clipped objective with KL penalty</li>
                                </ul>
                                
                                <p><strong>Memory Management:</strong></p>
                                <div class="equation math-jax" style="font-size: 0.8em; padding: 6px; margin: 6px 0;">
                                    $$ \text{Memory} = \text{Model Size} + B \times G \times L \times \text{Hidden Dim} $$
                                </div>
                                <p>Where L is sequence length. Use gradient checkpointing and mixed precision to reduce memory usage.</p>
                                
                                <p><strong>Common Implementation Pitfalls:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Division by zero:</strong> Add ε = 1e-8 to standard deviation</li>
                                    <li><strong>Gradient explosion:</strong> Clip gradients to max norm 1.0</li>
                                    <li><strong>Memory leaks:</strong> Clear intermediate tensors after each group</li>
                                    <li><strong>Numerical instability:</strong> Use log-space computations for probabilities</li>
                                </ul>
                                
                                <p><strong>Debugging Strategies:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Monitor advantage distribution:</strong> Should be roughly normal with mean ≈ 0</li>
                                    <li><strong>Track reward variance:</strong> Low variance indicates need for harder problems</li>
                                    <li><strong>Check clipping frequency:</strong> 10-30% clipping is healthy</li>
                                    <li><strong>Validate group independence:</strong> Ensure no data leakage between groups</li>
                                </ul>
                                
                                <p><strong>Hyperparameter Tuning Guide:</strong></p>
                                <ul style="font-size: 0.85em; margin: 8px 0;">
                                    <li><strong>Start conservative:</strong> Small learning rate, moderate group size</li>
                                    <li><strong>Increase gradually:</strong> Scale up as training stabilizes</li>
                                    <li><strong>Monitor KL divergence:</strong> Keep below 0.1 for stability</li>
                                    <li><strong>Adjust temperature:</strong> Higher for exploration, lower for exploitation</li>
                                </ul>
                            </div>
                            <div class="card-example">
                                <h6>Detailed LLM Example: Production Implementation</h6>
                                <strong>Scenario:</strong> Implementing GRPO for a production coding assistant<br><br>
                                <strong>System Architecture:</strong><br>
                                • Model: CodeLlama 13B<br>
                                • Infrastructure: 4x A100 GPUs<br>
                                • Group size: G = 8<br>
                                • Batch size: B = 32<br>
                                • Sequence length: 2048 tokens<br><br>
                                <strong>Memory Optimization:</strong><br>
                                • Gradient checkpointing: 40% memory reduction<br>
                                • Mixed precision (fp16): 50% memory reduction<br>
                                • Sequence packing: 20% efficiency improvement<br>
                                • Total memory usage: 18GB per GPU<br><br>
                                <strong>Performance Monitoring:</strong><br>
                                • Training throughput: 1.2 iterations/minute<br>
                                • Code correctness: Tracked via automated testing<br>
                                • Advantage variance: Monitored for curriculum adjustment<br>
                                • KL divergence: Kept below 0.05 for stability<br><br>
                                <strong>Common Issues Encountered:</strong><br>
                                1. <strong>Low reward variance:</strong> Problems too easy → increased difficulty<br>
                                2. <strong>High KL divergence:</strong> Learning rate too high → reduced to 1e-6<br>
                                3. <strong>Memory overflow:</strong> Batch size too large → reduced to 16<br>
                                4. <strong>Poor code quality:</strong> Added style and efficiency rewards<br><br>
                                <strong>Production Results:</strong><br>
                                • Code correctness: 87% → 94% after GRPO training<br>
                                • User satisfaction: 7.2/10 → 8.6/10<br>
                                • Training time: 2 weeks vs 6 weeks for PPO<br>
                                • Cost savings: 65% reduction in compute costs
                            </div>
                        </div>
                    </div>
                </div>
                <div class="card-navigation">
                    <button id="grpo-prev" disabled>Previous</button>
                    <div class="step-indicators">
                        <span class="dot active" data-step="1"></span>
                        <span class="dot" data-step="2"></span>
                        <span class="dot" data-step="3"></span>
                        <span class="dot" data-step="4"></span>
                        <span class="dot" data-step="5"></span>
                        <span class="dot" data-step="6"></span>
                    </div>
                    <button id="grpo-next">Next</button>
                </div>
            </div>

            <h4>Mathematical Deep Dive</h4>
            <p>For each prompt, GRPO samples a group of \(G\) outputs. The reward for each output \(o_i\) is \(r_i\). The key innovation is calculating the <strong>Advantage \(A_i\)</strong> by normalizing the rewards within the group:</p>
            <div class="equation math-jax">
                $$ A_i = \frac{r_i - \text{mean}(\{r_1, ..., r_G\})}{\text{std}(\{r_1, ..., r_G\})} $$
            </div>
            The GRPO objective then uses this advantage to update the policy, rewarding completions with above-average scores and penalizing those with below-average scores.
            <div class="equation math-jax">
                $$ \mathcal{J}_{\text{GRPO}}(\theta) = \mathbb{E} \left[ \sum_{i=1}^G \left( \text{ratio}(\theta, o_i) \cdot A_i - \beta \cdot \text{KL} \right) \right] $$
            </div>
            
            <div class="example-box">
                <h4>Detailed LLM Example: Math Reasoning Assistant</h4>
                <p><strong>Scenario:</strong> Training an AI math tutor using GRPO to solve algebra problems</p>
                
                <p><strong>Student Question:</strong> "Solve for x: 2x + 3 = 11"</p>
                
                <p><strong>GRPO Group Generation:</strong> The model generates 3 different reasoning approaches simultaneously:</p>
                
                <p><strong>Response 1 (Detailed Step-by-Step):</strong><br>
                "Let me solve this equation step by step. First, I'll subtract 3 from both sides: 2x + 3 - 3 = 11 - 3, which gives me 2x = 8. Then I'll divide both sides by 2: 2x ÷ 2 = 8 ÷ 2, so x = 4. Let me verify: 2(4) + 3 = 8 + 3 = 11 ✓"<br>
                • Clear methodology, shows verification<br>
                • <strong>Reward: \(r_1 = 1.0\)</strong> (Correct answer + excellent explanation)</p>
                
                <p><strong>Response 2 (Concise but Correct):</strong><br>
                "I need to isolate x. 2x + 3 = 11, so 2x = 11 - 3 = 8, therefore x = 8 ÷ 2 = 4."<br>
                • Efficient, correct steps<br>
                • <strong>Reward: \(r_2 = 1.0\)</strong> (Correct answer + good reasoning)</p>
                
                <p><strong>Response 3 (Common Student Error):</strong><br>
                "To solve 2x + 3 = 11, I'll add 3 to both sides: 2x = 11 + 3 = 14, so x = 14 ÷ 2 = 7."<br>
                • Shows common algebraic mistake (adding instead of subtracting)<br>
                • <strong>Reward: \(r_3 = 0.0\)</strong> (Incorrect answer due to sign error)</p>
                
                <p><strong>GRPO Advantage Calculation:</strong></p>
                <ul>
                    <li><strong>Group Statistics:</strong><br>
                        - Mean reward: \((1.0 + 1.0 + 0.0) ÷ 3 = 0.67\)<br>
                        - Standard deviation: \(\sqrt{\frac{(1.0-0.67)^2 + (1.0-0.67)^2 + (0.0-0.67)^2}{3}} = 0.47\)</li>
                    <li><strong>Normalized Advantages:</strong><br>
                        - \(A_1 = (1.0 - 0.67) ÷ 0.47 = +0.70\) (Response 1: above average)<br>
                        - \(A_2 = (1.0 - 0.67) ÷ 0.47 = +0.70\) (Response 2: above average)<br>
                        - \(A_3 = (0.0 - 0.67) ÷ 0.47 = -1.43\) (Response 3: well below average)</li>
                </ul>
                
                <p><strong>Learning Outcome:</strong><br>
                • Model strongly reinforces correct algebraic procedures (both detailed and concise)<br>
                • Model learns to avoid the common sign error in equation solving<br>
                • Future responses will favor systematic approaches and verification steps<br>
                • The group comparison helps the model understand that multiple correct approaches exist</p>
                
                <p><strong>Key GRPO Insight:</strong> By comparing responses within the same group, the model learns relative quality without needing a separate critic model. The normalization ensures that even when all responses are good (or all are bad), the model still gets meaningful learning signals.</p>
            </div>
        </div>
        
        <div class="container">
            <h2>Summary Comparison Table</h2>
            <table>
                <thead>
                    <tr>
                        <th>Aspect</th>
                        <th>PPO (Proximal Policy Optimization)</th>
                        <th>DPO (Direct Preference Optimization)</th>
                        <th>GRPO (Group Relative Policy Optimization)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Core Idea</strong></td>
                        <td>Maximize a reward signal from a learned model while staying close to the original policy.</td>
                        <td>Directly optimize the policy on preference pairs, bypassing an explicit reward model.</td>
                        <td>Normalize rewards within a group of responses to create a learning signal without a critic model.</td>
                    </tr>
                    <tr>
                        <td><strong>Objective Function</strong></td>
                        <td class="math-cell math-jax" style="font-size:0.75em;">$$ L^{PPO} = \mathbb{E}[\min(r_t \hat{A}_t, \text{clip}(...) \hat{A}_t) - c_1 L^{VF} + c_2 S] $$</td>
                        <td class="math-cell math-jax" style="font-size:0.75em;">$$ L^{DPO} = -\mathbb{E}\left[\log \sigma\left(\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\right)\right] $$</td>
                        <td class="math-cell math-jax" style="font-size:0.75em;">$$ \mathcal{J}^{GRPO} = \mathbb{E} \left[ \frac{1}{G}\sum_{i=1}^G \left( \min(r_i A_i, \text{clip}(...)A_i) - \beta \mathbb{D}_{KL} \right) \right] $$</td>
                    </tr>
                    <tr>
                        <td><strong>Reward / Preference Model</strong></td>
                        <td>Requires an explicit, separately trained reward model \(r_\phi(x,y)\) that predicts human scores.</td>
                        <td>Implicitly models preferences using the Bradley-Terry model. No separate reward model is trained.</td>
                        <td>Uses any external reward function \(R(x,y)\), which is often simple and rule-based (e.g., correct/incorrect).</td>
                    </tr>
                    <tr>
                        <td><strong>Advantage Calculation</strong></td>
                        <td class="math-cell math-jax" style="font-size:0.9em;">GAE: \( \hat{A}_t = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l} \), where \(\delta_t\) is the TD error from the critic.</td>
                        <td class="math-cell math-jax" style="font-size:0.9em;">Implicit reward diff: \( \hat{r}_w - \hat{r}_l = \beta \log \frac{\pi(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \dots \)</td>
                        <td class="math-cell math-jax" style="font-size:0.9em;">Group-wise normalization: \( A_i = \frac{r_i - \mu_G}{\sigma_G + \epsilon} \). Critic-free.</td>
                    </tr>
                    <tr>
                        <td><strong>Key Innovation</strong></td>
                        <td>Clipped surrogate objective provides stability of TRPO with first-order optimization.</td>
                        <td>Analytically maps the RLHF objective to a simple classification loss on preference data.</td>
                        <td>Replaces the expensive critic with cheap, in-batch normalization of rewards.</td>
                    </tr>
                    <tr>
                        <td><strong>Data Requirement</strong></td>
                        <td>Prompts and responses for RL sampling. Reward model requires preference pairs \((x, y_w, y_l)\).</td>
                        <td>Requires a dataset of preference triplets: \((x, y_w, y_l)\).</td>
                        <td>Requires prompts and a reward function. No human preference data is strictly necessary.</td>
                    </tr>
                    <tr>
                        <td><strong>Computational Cost</strong></td>
                        <td><strong>Very High:</strong> Trains policy + critic + reward models. Requires expensive online sampling.</td>
                        <td><strong>Low:</strong> Simple offline training on a fixed dataset. No sampling loop.</td>
                        <td><strong>Medium:</strong> Cheaper than PPO (no critic), but requires online sampling which is costly.</td>
                    </tr>
                    <tr>
                        <td><strong>Hyperparameter Tuning</strong></td>
                        <td><strong>Difficult:</strong> Very sensitive to learning rates, clipping epsilon, GAE params, and KL coefficient.</td>
                        <td><strong>Easy:</strong> Primarily sensitive to \(\beta\) and learning rate. Very stable.</td>
                        <td><strong>Medium:</strong> Sensitive to learning rate, group size G, and KL coefficient. More stable than PPO.</td>
                    </tr>
                    <tr>
                        <td><strong>Common Failure Modes</strong></td>
                        <td><strong>Reward Hacking:</strong> Finds loopholes in the reward model. <br><strong>Instability:</strong> Policy can collapse or generate gibberish.</td>
                        <td><strong>Overfitting:</strong> Can overfit to the specific preferences in the dataset. <br><strong>Stale Reference:</strong> Performance depends on a good \(\pi_{ref}\).</td>
                        <td><strong>Low Variance Collapse:</strong> If all responses in a group are identical, learning signal becomes zero.</td>
                    </tr>
                    <tr>
                        <td><strong>Scalability</strong></td>
                        <td>Scales well but is very resource-intensive. Used for large models like ChatGPT.</td>
                        <td>Scales very well; simple loss is efficient for large datasets and models.</td>
                        <td>Extremely scalable for reasoning tasks, as shown by DeepSeek-R1. Efficiency is a key advantage.</td>
                    </tr>
                    <tr>
                        <td><strong>Best Use Case</strong></td>
                        <td>General-purpose alignment where rewards are complex and subjective (e.g., conversational quality).</td>
                        <td>Stable and efficient fine-tuning when high-quality preference pairs are available.</td>
                        <td>Improving reasoning on tasks with clear, objective rewards (e.g., math, coding, logic).</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="container">
            <h2>4. DeepSeek-R1: A Case Study in Advanced Reasoning</h2>
            <p>The DeepSeek-R1 paper showcases a masterclass in applying these alignment techniques to achieve state-of-the-art reasoning capabilities. Their approach involves multiple, carefully orchestrated stages that build on each other, moving from pure, unguided reinforcement learning to highly refined, multi-faceted alignment.</p>

            <h3>The Full DeepSeek-R1 Pipeline</h3>
            <div class="full-pipeline-container">
                <div class="pipeline-stage">
                    <h4>Stage 1: DeepSeek-R1-Zero</h4>
                    <div class="workflow-mini">
                        <div class="workflow-step highlight-start">Start with Base Model (DeepSeek-V3)</div>
                        <div class="workflow-arrow-down">&darr;</div>
                        <div class="workflow-step highlight-loop">Pure GRPO Training with Rule-Based Rewards (Accuracy + Format)</div>
                        <div class="workflow-arrow-down">&darr;</div>
                        <div class="workflow-step highlight-out"><b>Output: DS-R1-Zero</b><br/>(Powerful but messy)</div>
                    </div>
                </div>
                <div class="pipeline-arrow">&rarr;</div>
                <div class="pipeline-stage">
                    <h4>Stage 2: Cold Start SFT</h4>
                     <div class="workflow-mini">
                        <div class="workflow-step highlight-start">Base Model (DeepSeek-V3)</div>
                        <div class="workflow-arrow-down">&darr;</div>
                        <div class="workflow-step highlight-data">Collect thousands of high-quality, "cold-start" reasoning examples</div>
                        <div class="workflow-arrow-down">&darr;</div>
                        <div class="workflow-step highlight-sft">Supervised Fine-Tuning (SFT)</div>
                         <div class="workflow-arrow-down">&darr;</div>
                         <div class="workflow-step highlight-out"><b>Output: Initial SFT Model</b></div>
                     </div>
                </div>
                <div class="pipeline-arrow">&rarr;</div>
                <div class="pipeline-stage">
                    <h4>Stage 3 & 4: Iterative RL and SFT</h4>
                    <div class="workflow-mini">
                        <div class="workflow-step highlight-loop">Reasoning-Oriented GRPO</div>
                        <div class="workflow-arrow-down">&darr;</div>
                        <div class="workflow-step highlight-data">Rejection Sampling to create new SFT data (+ General & Safety Data)</div>
                        <div class="workflow-arrow-down">&darr;</div>
                        <div class="workflow-step highlight-sft">Second SFT Stage</div>
                        <div class="workflow-arrow-down">&darr;</div>
                        <div class="workflow-step highlight-loop">Final GRPO for All Scenarios</div>
                        <div class="workflow-arrow-down">&darr;</div>
                        <div class="workflow-step final"><b>DeepSeek-R1</b></div>
                    </div>
                </div>
            </div>

            <h3>Stage 1: DeepSeek-R1-Zero - Pure Reinforcement Learning</h3>
            <p>The first experiment, `DS-R1-Zero`, tested a bold hypothesis: can an LLM develop reasoning abilities through pure RL without any initial supervised fine-tuning? The answer was a resounding yes.</p>
            <ul>
                <li><strong>Method:</strong> They applied GRPO directly to the base DeepSeek-V3 model.</li>
                <li><strong>Rewards:</strong> The reward functions were simple and rule-based:
                    <ol>
                        <li><strong>Accuracy Reward:</strong> Was the final answer, extracted from a specific format (e.g., a box), mathematically or logically correct? (Reward = 1.0 or 0.0).</li>
                        <li><strong>Format Reward:</strong> Did the model correctly use the `<think>` and `</think>` tags to wrap its reasoning process?</li>
                    </ol>
                </li>
                <li><strong>Outcome:</strong> `DS-R1-Zero` achieved remarkable reasoning scores, proving that complex behaviors like self-verification and generating long chains of thought can emerge autonomously from a simple RL setup. However, its outputs were often messy and hard for humans to read.</li>
            </ul>

            <h4>Mathematical Deep Dive: GRPO</h4>
            <p>The core of GRPO is its critic-free objective function. For each prompt \(q\), a group of outputs \(\{o_1, ..., o_G\}\) are sampled. The advantage \(A_i\) for each output is calculated by normalizing its reward against the group's statistics.</p>
            <div class="equation math-jax">
                $$ A_i = \frac{r_i - \text{mean}(\{r_1, ..., r_G\})}{\text{std}(\{r_1, ..., r_G\})} $$
            </div>
            <p>The full GRPO objective function \(\mathcal{J}_{GRPO}\) then maximizes the expected advantage, clipped to prevent unstable updates, while regularized by a KL-divergence term.</p>
            <div class="equation math-jax" style="font-size: 1em;">
                $$ \mathcal{J}_{GRPO}(\theta) = \mathbb{E} \left[ \frac{1}{G}\sum_{i=1}^G \left( \min \left( \frac{\pi_\theta(o_i |q)}{\pi_{\theta_{old}}(o_i |q)} A_i, \text{clip} \left( \frac{\pi_\theta(o_i |q)}{\pi_{\theta_{old}}(o_i |q)}, 1 - \epsilon, 1 + \epsilon \right)  A_i \right) - \beta \mathbb{D}_{KL} \right) \right] $$
            </div>
            <ul>
                <li>\(\pi_\theta\) is the policy being trained.</li>
                <li>\(\pi_{\theta_{old}}\) is the fixed policy used for sampling the outputs.</li>
                <li>The ratio \(\frac{\pi_\theta}{\pi_{\theta_{old}}}\) is the importance sampling weight.</li>
                <li>\(\text{clip}(...)\) is the standard PPO clipping function.</li>
                <li>\(\mathbb{D}_{KL}\) is a KL-divergence penalty keeping \(\pi_\theta\) close to a reference policy.</li>
            </ul>

            <h3>Stage 2: The Multi-Stage Pipeline for DeepSeek-R1</h3>
            <p>To address the shortcomings of `DS-R1-Zero` and push performance even further, a more structured, multi-stage pipeline was introduced for `DS-R1`.</p>
            
            <h4>A. Cold Start SFT</h4>
            <p>Instead of starting with pure RL, this process begins by giving the model a "cold start"—fine-tuning it on a small, high-quality dataset of ideal reasoning examples. This initial SFT helps the model learn a clean, readable, and effective reasoning format from the outset.</p>
            <div class="example-box">
                <h4>Detailed LLM Example: Cold Start Data Creation for Reasoning</h4>
                <p><strong>Scenario:</strong> Creating high-quality reasoning examples for DeepSeek-R1's initial SFT phase</p>
                
                <p><strong>Target Skill:</strong> Mathematical reasoning with clear thought processes</p>
                
                <p><strong>Sample Problem:</strong> "What is 15% of 60?"</p>
                
                <p><strong>Few-Shot Prompting Strategy:</strong> Show the model examples of ideal reasoning format</p>
                
                <div class="code-block">
                    <strong>Example 1:</strong><br>
                    Q: What is 20% of 50?<br>
                    A: &lt;think&gt;I need to calculate 20% of 50. First, I'll convert the percentage to a decimal: 20% = 0.20. Now I multiply: 0.20 × 50 = 10. Let me double-check: 20% means 20 out of 100, so 20/100 × 50 = 1000/100 = 10. ✓&lt;/think&gt;&lt;answer&gt;10&lt;/answer&gt;<br><br>
                    
                    <strong>Example 2:</strong><br>
                    Q: What is 25% of 80?<br>
                    A: &lt;think&gt;25% of 80. I can think of this as 1/4 of 80 since 25% = 25/100 = 1/4. So 80 ÷ 4 = 20. Alternatively, 0.25 × 80 = 20. Both methods give the same answer.&lt;/think&gt;&lt;answer&gt;20&lt;/answer&gt;<br><br>
                    
                    <strong>Now the target problem:</strong><br>
                    Q: What is 15% of 60?<br>
                    A:
                </div>
                
                <p><strong>Generated High-Quality Response:</strong></p>
                <div class="code-block">
                    &lt;think&gt;I need to find 15% of 60. Let me convert 15% to a decimal: 15% = 0.15. Now I'll multiply: 0.15 × 60. I can break this down: 0.10 × 60 = 6, and 0.05 × 60 = 3. So 0.15 × 60 = 6 + 3 = 9. Let me verify using fractions: 15% = 15/100 = 3/20. So (3/20) × 60 = (3 × 60)/20 = 180/20 = 9. ✓&lt;/think&gt;&lt;answer&gt;9&lt;/answer&gt;
                </div>
                
                <p><strong>Quality Features of This Example:</strong></p>
                <ul>
                    <li><strong>Clear reasoning process:</strong> Shows step-by-step calculation</li>
                    <li><strong>Multiple methods:</strong> Demonstrates both decimal and fraction approaches</li>
                    <li><strong>Verification:</strong> Checks the answer using an alternative method</li>
                    <li><strong>Proper formatting:</strong> Uses &lt;think&gt; tags for reasoning, &lt;answer&gt; for final result</li>
                    <li><strong>Educational value:</strong> Explains the mathematical concepts clearly</li>
                </ul>
                
                <p><strong>SFT Training Impact:</strong><br>
                This example teaches the model to:<br>
                • Always show detailed reasoning in &lt;think&gt; tags<br>
                • Use multiple solution approaches when possible<br>
                • Verify answers through alternative methods<br>
                • Structure responses clearly with proper formatting<br>
                • Explain mathematical concepts in an educational manner</p>
                
                <p><strong>Scaling Up:</strong> Thousands of such high-quality examples across various mathematical topics (algebra, geometry, calculus, word problems) form the foundation for DeepSeek-R1's reasoning capabilities.</p>
            </div>

            <h4>B. Iterative RL and SFT</h4>
            <p>This is the core of the `DS-R1` training, involving several cycles of RL and data curation.</p>
            <ol>
                <li><strong>Reasoning-Oriented RL:</strong> The SFT model is trained using GRPO, similar to `DS-R1-Zero`, but now with an additional reward for language consistency to ensure readable outputs.</li>
                <li><strong>Rejection Sampling & SFT Data Curation:</strong> After the first RL stage, the now-powerful model is used to generate a large amount of reasoning data. This is where <strong>rejection sampling</strong> comes in.</li>
            </ol>

            <p><strong>Rejection Sampling:</strong> For each prompt, generate many possible responses. Keep only the ones that are verified to be correct. This creates a new, high-quality dataset for the next stage.</p>
            <div class="example-box">
                <h4>Detailed LLM Example: Rejection Sampling for Logic Puzzles</h4>
                <p><strong>Scenario:</strong> Creating high-quality reasoning data through rejection sampling</p>
                
                <p><strong>Classic Logic Problem:</strong> "A bat and a ball cost $1.10 total. The bat costs $1.00 more than the ball. How much does the ball cost?"</p>
                
                <p><strong>GRPO Group Generation (4 responses):</strong></p>
                
                <p><strong>Response 1 (Common Intuitive Error):</strong></p>
                <div class="code-block">
                    &lt;think&gt;The bat and ball cost $1.10 total. The bat costs $1.00 more than the ball. So if the ball costs x, then the bat costs x + $1.00. But wait, that would mean the bat costs $1.00 and the ball costs $0.10, totaling $1.10.&lt;/think&gt;&lt;answer&gt;$0.10&lt;/answer&gt;
                </div>
                <p>• <strong>Reward: 0</strong> (Incorrect - falls into the common intuitive trap)</p>
                
                <p><strong>Response 2 (Correct Algebraic Solution):</strong></p>
                <div class="code-block">
                    &lt;think&gt;Let me set up equations. Let b = cost of ball. Then bat costs b + $1.00. Total: b + (b + $1.00) = $1.10. So 2b + $1.00 = $1.10. Therefore 2b = $0.10, which means b = $0.05. Let me verify: ball = $0.05, bat = $0.05 + $1.00 = $1.05. Total: $0.05 + $1.05 = $1.10 ✓&lt;/think&gt;&lt;answer&gt;$0.05&lt;/answer&gt;
                </div>
                <p>• <strong>Reward: 1</strong> (Correct with proper algebraic reasoning and verification)</p>
                
                <p><strong>Response 3 (Correct Answer, Format Issue):</strong></p>
                <div class="code-block">
                    &lt;think&gt;If the ball is x and bat is x + 1, then x + x + 1 = 1.10, so 2x = 0.10, so x = 0.05&lt;/think&gt;&lt;answer&gt;The ball costs 5 cents&lt;/answer&gt;
                </div>
                <p>• <strong>Reward: 0</strong> (Correct reasoning but format inconsistency - "5 cents" vs "$0.05")</p>
                
                <p><strong>Response 4 (Detailed Step-by-Step):</strong></p>
                <div class="code-block">
                    &lt;think&gt;This is a classic algebra problem. Let's define variables: let b = price of ball in dollars. Then the bat costs b + 1.00 dollars. The total is: b + (b + 1.00) = 1.10. Simplifying: 2b + 1.00 = 1.10. Subtracting 1.00 from both sides: 2b = 0.10. Dividing by 2: b = 0.05. So the ball costs $0.05 and the bat costs $0.05 + $1.00 = $1.05. Check: $0.05 + $1.05 = $1.10 ✓&lt;/think&gt;&lt;answer&gt;The ball costs $0.05 and the bat costs $1.05&lt;/answer&gt;
                </div>
                <p>• <strong>Reward: 1</strong> (Excellent detailed reasoning with both individual and total answers)</p>
                
                <p><strong>GRPO Advantage Calculation:</strong></p>
                <ul>
                    <li>Rewards: [0, 1, 0, 1]</li>
                    <li>Mean: 0.5, Standard deviation: 0.5</li>
                    <li>Advantages: [-1.0, +1.0, -1.0, +1.0]</li>
                </ul>
                
                <p><strong>Rejection Sampling Result:</strong><br>
                Only responses 2 and 4 are kept for the next SFT dataset because they received reward = 1. This creates a high-quality dataset containing only successful reasoning patterns.</p>
                
                <p><strong>Learning Benefits:</strong></p>
                <ul>
                    <li><strong>Eliminates common errors:</strong> Model learns to avoid the intuitive $0.10 trap</li>
                    <li><strong>Reinforces systematic approach:</strong> Algebraic setup → solve → verify</li>
                    <li><strong>Maintains format consistency:</strong> Standardizes on dollar notation</li>
                    <li><strong>Encourages verification:</strong> Always check the final answer</li>
                </ul>
                
                <p><strong>Scaling Impact:</strong> This process, repeated across thousands of logic puzzles, math problems, and reasoning tasks, creates a curated dataset of only the highest-quality reasoning chains for subsequent SFT training.</p>
            </div>
            <p>This new dataset, now containing hundreds of thousands of high-quality reasoning samples, is combined with general-purpose data (for conversation, writing, etc.) and safety data. A <strong>second SFT</strong> is performed on the base model with this enriched dataset.</p>

            <li><strong>Final RL for All Scenarios:</strong> The model undergoes one final GRPO stage. This time, the training data includes not just reasoning tasks, but a mix of all scenarios (general conversation, helpfulness, harmlessness) to produce a well-rounded and robust final model: **DeepSeek-R1**.</li>

            <h3>C. Distillation</h3>
            <p>The final step mentioned in the paper is distillation. The massive, powerful `DeepSeek-R1` model is used as a "teacher" to train smaller, more efficient "student" models. The student models (e.g., a 7B parameter model) are trained via SFT to mimic the high-quality reasoning outputs of the teacher. This makes state-of-the-art reasoning accessible without requiring enormous computational resources, a key contribution to the community.</p>
        </div>
    </div>
    
    <footer>
        <p>End of Lecture | LLM Alignment Techniques</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            // PPO Interactive Workflow
            const ppoPrevBtn = document.getElementById('ppo-prev');
            const ppoNextBtn = document.getElementById('ppo-next');
            const ppoCards = document.querySelectorAll('#ppo-interactive-workflow .workflow-card');
            const ppoDots = document.querySelectorAll('#ppo-interactive-workflow .dot');
            let ppoCurrentStep = 1;
            const ppoTotalSteps = ppoCards.length;

            function updatePPOWorkflowView() {
                ppoCards.forEach(card => {
                    if (parseInt(card.dataset.step) === ppoCurrentStep) {
                        card.classList.add('active');
                    } else {
                        card.classList.remove('active');
                    }
                });

                ppoDots.forEach(dot => {
                    if (parseInt(dot.dataset.step) === ppoCurrentStep) {
                        dot.classList.add('active');
                    } else {
                        dot.classList.remove('active');
                    }
                });

                ppoPrevBtn.disabled = ppoCurrentStep === 1;
                ppoNextBtn.disabled = ppoCurrentStep === ppoTotalSteps;
            }

            if (ppoPrevBtn) {
                ppoNextBtn.addEventListener('click', () => {
                    if (ppoCurrentStep < ppoTotalSteps) {
                        ppoCurrentStep++;
                        updatePPOWorkflowView();
                    }
                });
        
                ppoPrevBtn.addEventListener('click', () => {
                    if (ppoCurrentStep > 1) {
                        ppoCurrentStep--;
                        updatePPOWorkflowView();
                    }
                });
        
                updatePPOWorkflowView();
            }

            // DPO Interactive Workflow
            const dpoPrevBtn = document.getElementById('dpo-prev');
            const dpoNextBtn = document.getElementById('dpo-next');
            const dpoCards = document.querySelectorAll('#dpo-interactive-workflow .workflow-card');
            const dpoDots = document.querySelectorAll('#dpo-interactive-workflow .dot');
            let dpoCurrentStep = 1;
            const dpoTotalSteps = dpoCards.length;

            function updateDPOWorkflowView() {
                dpoCards.forEach(card => {
                    if (parseInt(card.dataset.step) === dpoCurrentStep) {
                        card.classList.add('active');
                    } else {
                        card.classList.remove('active');
                    }
                });

                dpoDots.forEach(dot => {
                    if (parseInt(dot.dataset.step) === dpoCurrentStep) {
                        dot.classList.add('active');
                    } else {
                        dot.classList.remove('active');
                    }
                });

                dpoPrevBtn.disabled = dpoCurrentStep === 1;
                dpoNextBtn.disabled = dpoCurrentStep === dpoTotalSteps;
            }

            if (dpoPrevBtn) {
                dpoNextBtn.addEventListener('click', () => {
                    if (dpoCurrentStep < dpoTotalSteps) {
                        dpoCurrentStep++;
                        updateDPOWorkflowView();
                    }
                });
        
                dpoPrevBtn.addEventListener('click', () => {
                    if (dpoCurrentStep > 1) {
                        dpoCurrentStep--;
                        updateDPOWorkflowView();
                    }
                });
        
                updateDPOWorkflowView();
            }

            // GRPO Interactive Workflow
            const grpoPrevBtn = document.getElementById('grpo-prev');
            const grpoNextBtn = document.getElementById('grpo-next');
            const grpoCards = document.querySelectorAll('#grpo-interactive-workflow .workflow-card');
            const grpoDots = document.querySelectorAll('#grpo-interactive-workflow .dot');
            let grpoCurrentStep = 1;
            const grpoTotalSteps = grpoCards.length;

            function updateGRPOWorkflowView() {
                grpoCards.forEach(card => {
                    if (parseInt(card.dataset.step) === grpoCurrentStep) {
                        card.classList.add('active');
                    } else {
                        card.classList.remove('active');
                    }
                });

                grpoDots.forEach(dot => {
                    if (parseInt(dot.dataset.step) === grpoCurrentStep) {
                        dot.classList.add('active');
                    } else {
                        dot.classList.remove('active');
                    }
                });

                grpoPrevBtn.disabled = grpoCurrentStep === 1;
                grpoNextBtn.disabled = grpoCurrentStep === grpoTotalSteps;
            }

            if (grpoPrevBtn) {
                grpoNextBtn.addEventListener('click', () => {
                    if (grpoCurrentStep < grpoTotalSteps) {
                        grpoCurrentStep++;
                        updateGRPOWorkflowView();
                    }
                });
        
                grpoPrevBtn.addEventListener('click', () => {
                    if (grpoCurrentStep > 1) {
                        grpoCurrentStep--;
                        updateGRPOWorkflowView();
                    }
                });
        
                updateGRPOWorkflowView();
            }
        });
    </script>
</body>
</html>
